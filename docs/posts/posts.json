[
  {
    "path": "posts/2021-02-21-journal1/",
    "title": "Journal 1",
    "description": "Getting started with R, RStudio and RMarkdown.",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2025-02-21",
    "categories": [],
    "contents": "\r\nGive a fish, teach to fish\r\nSetting up a GitHub-enabled version control project in RStudio was initially frustrating, like many coding experiences before it. The instructions provided by Joe and Johannes seemed deceptively simple. I have come to understand that this is a general principle in coding pedagogy; “Give a [person] a fish and you feed [them] for a day. Teach a [person] to fish and you feed [them] for a lifetime.”\r\nDespite the frustration, being forced to fill in the gaps between instructions enabled me to have a better understanding than if it had been laid out exactingly. After downloading Git and being unable to find the .exe file, Johannes helpfully pointed out that I had not followed the sacred code of the helpdesk technician: “Have you tried turning it on and off again?” Rather shamefacedly I managed to link my repository.\r\nAnother issue was creating an SSH key which I had not done before. The GitHub guide was not overly helpful in advising me to copy the public SSH key I’d created in Git Bash, for the simple reason that it did not identify what file the key was saved to, nor what character length the key should resemble. Some quick Googling and scanning Stack Overflow revealed the answer, to great relief. Unfortunately I have not yet had the opportunity to return the favour and feed the multitude with my own paltry offerings of bread and fish knowledge, despite having the fervour of a pseudo-religious convert to these coding development tools.\r\nMore generally, I am excited for what this course is intended to prepare us with. The ability to conduct advanced data analytic techniques with today’s home computing power is unprecedented, leading to new ways of open collaboration. The power of quantitative research is illustrated in the collapse of Northern Cod stocks in Atlantic Canada in the 1990s (see Figure 1).\r\n\r\nCollapse of Atlantic Cod Stocks off the East Coast of Newfoundland in 1992 (Millennium Ecosystem Assessment, 2005)\r\nThe population size at the time is argued to have been overestimated due to modelling of catch rate data from modern vessels equipped with vastly superior technology than in previous decades (Hutchings & Myers, 1994). The resulting sample error from abundant harvests led to overconfidence that fishery stocks were plentiful when in fact they were at dangerously low levels. The ego has been a fatality both for the human self and for Cod.1\r\n\r\n\r\n\r\nHutchings, J. A., & Myers, R. A. (1994). What can be learned from the collapse of a renewable resource? Atlantic cod, gadus morhua, of newfoundland and labrador. Canadian Journal of Fisheries and Aquatic Sciences, 51(9), 2126–2146.\r\n\r\n\r\nMillennium Ecosystem Assessment. (2005). Ecosystems and human well-being: synthesis. Washington, DC: Island Press.\r\n\r\n\r\nThis was once revealed to me in a bream.↩︎\r\n",
    "preview": {},
    "last_modified": "2021-05-12T20:42:45+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-21-workbook1/",
    "title": "Workbook 1",
    "description": "Getting started with R, RStudio and RMarkdown.",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2025-02-21",
    "categories": [],
    "contents": "\r\nGive a fish, teach to fish\r\nSetting up a GitHub-enabled version control project in RStudio was initially frustrating, like many coding experiences before it. The instructions provided by Joe and Johannes seemed deceptively simple. I have come to understand that this is a general principle in coding pedagogy; “Give a [person] a fish and you feed [them] for a day. Teach a [person] to fish and you feed [them] for a lifetime.”\r\nDespite the frustration, being forced to fill in the gaps between instructions enabled me to have a better understanding than if it had been laid out exactingly. After downloading Git and being unable to find the .exe file, Johannes helpfully pointed out that I had not followed the sacred code of the helpdesk technician: “Have you tried turning it on and off again?” Rather shamefacedly I managed to link my repository.\r\nAnother issue was creating an SSH key which I had not done before. The GitHub guide was not overly helpful in advising me to copy the public SSH key I’d created in Git Bash, for the simple reason that it did not identify what file the key was saved to, nor what character length the key should resemble. Some quick Googling and scanning Stack Overflow revealed the answer, to great relief. Unfortunately I have not yet had the opportunity to return the favour and feed the multitude with my own paltry offerings of bread and fish knowledge, despite having the fervour of a pseudo-religious convert to these coding development tools.\r\nMore generally, I am excited for what this course is intended to prepare us with. The ability to conduct advanced data analytic techniques with today’s home computing power is unprecedented, leading to new ways of open collaboration. The power of quantitative research is illustrated in the collapse of Northern Cod stocks in Atlantic Canada in the 1990s (see Figure 1).\r\n\r\nCollapse of Atlantic Cod Stocks off the East Coast of Newfoundland in 1992 (Millennium Ecosystem Assessment, 2005)\r\nThe population size at the time is argued to have been overestimated due to modelling of catch rate data from modern vessels equipped with vastly superior technology than in previous decades (Hutchings & Myers, 1994). The resulting sample error from abundant harvests led to overconfidence that fishery stocks were plentiful when in fact they were at dangerously low levels. The ego has been a fatality both for the human self and for Cod.1\r\n\r\n\r\n\r\nHutchings, J. A., & Myers, R. A. (1994). What can be learned from the collapse of a renewable resource? Atlantic cod, gadus morhua, of newfoundland and labrador. Canadian Journal of Fisheries and Aquatic Sciences, 51(9), 2126–2146.\r\n\r\n\r\nMillennium Ecosystem Assessment. (2005). Ecosystems and human well-being: synthesis. Washington, DC: Island Press.\r\n\r\n\r\nThis was once revealed to me in a bream.↩︎\r\n",
    "preview": {},
    "last_modified": "2021-06-06T15:30:00+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-07-finalproject/",
    "title": "Final Report",
    "description": "Prototypicality, discrimination and Maori identity",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-06-07",
    "categories": [],
    "contents": "\r\nDoes prototypicality of group membership moderate the effect of intergroup and intragroup discrimination on ethnic identity in Māori?\r\nMāori culture has been posited (e.g. by Durie, 1998, 2001) as a protective factor for psychological wellbeing. Strengthening cultural/ethnic identity may be a means to addessing disparities in health, education and other contexts. However, not all people who identify as having whakapapa Māori (Māori ancestry) have the access, means or desire to adopt a traditional, “fixed” Māori identity (McIntosh, 2005). In this project I will examine the heterogeneity of Māori cultural/ethnic identity formation through participants’ experiences of discrimination by Pākehā/Tauiwi and Māori.\r\nBranscombe, Schmitt and Harvey’s (1999) rejection-identification model proposes that perceived discrimination by the dominant group leads to strengthened ingroup identification, as a defense mechanism to protect the individual’s psychological wellbeing. Evidence for this model is mixed with some studies showing increased ethnic identity while others do not (Chávez & French, 2007; Evans et al., 2014; Masuoka, 2006; Pérez et al., 2008; Romero & Roberts, 2003). One explanation for these mixed results may be that some individuals are more or less able to traverse boundaries of group category membership because of how prototypical they appear or behave. Evidence suggests that self-rated similarity to the average ingroup member mediates the relationship between perceived discrimination and life satisfaction [giamo-2012]. However, research to date has not examined the effect of group membership prototypicality on the relationship between perceived discrimination and ethnic identity. If highly prototypical group members are less able than low prototypicality members to distance themselves from the group category for which they are being targeted for discrimination, then they may have more reason to defend whatever limited identity choices are available to them.\r\nI want to explore whether the diversity of participants’ self-rated prototypicality to the group category ‘Māori’ based on physical, cultural and social characteristics affects how discrimination experiences are rejected or internalised through strengthened or decreased identification as Māori. The main research questions are:\r\nDoes prototypicality of group membership moderate the effect of perceived discrimination by Pākehā/Tauiwi, on Māori identity?\r\nDoes prototypicality of group membership moderate the effect of perceived discrimination by Māori, on Māori identity?\r\nMethod\r\nParticipants\r\nParticipants were 669 members of the community who identified as Māori (62.9% female, 37.1% male; Meanage=44.4, SDage=13) recruited through the New Zealand Attitudes and Values Survey.\r\nMaterial\r\nParticipants completed a range of demographic questions and construct scales. The constructs used in the current study are included below.\r\nPerceived discrimination. Participants were asked to evaluate the single item, “I feel that I am often discriminated against because of my ethnicity” on a scale from 1 (Strongly Disagree) to 7 (Strongly Agree).\r\nPerceived appearance. This subscale of Houkamau and Sibley’s (2015) MMM-ICE2 examines self-rated prototypicality of physical appearance to the Māori ethnic group. Seven items including “I think it is easy to tell that I am Māori just by looking at me” and “People would never know that I am of Māori descent just by looking at me” are rated on a scale from 1 (Strongly Disagree) to 7 (Strongly Agree).\r\nGroup membership and evaluation. Another subscale of the MMM-ICE2 examines affective and centrality components of ethnic group membership (i.e. identity). Eight items including “I love the fact I am Māori” and “My Māori ancestry is important to me” are rated on a scale from 1 (Strongly Disagree) to 7 (Strongly Agree).\r\nProcedure\r\nParticipants were posted a copy of the questionnaire, with a second postal follow-up two months later. Participants who provided an email address were also emailed and invited to complete an online version if they preferred.\r\nTo boost sample size and increase sample diversity for subsequent waves, booster samples using different sample frames were conducted. Booster sampling was conducted without replacement (i.e., all people included in previous sample frames were identified and removed from the electoral roll before generation of the new sample frames).\r\nThe fifth sample frame consisted of 9,000 people randomly selected from those who indicated on the 2012 Electoral Roll that they were of Māori ethnicit y (ethnic affiliation as Māori is listed on the roll, but other ethnic affiliations are not). A total of 689 participants responded to this booster sample (adjusted response rate = 7.78%). The questionnaire administered to the Māori booster sample included questions specifically designed for Māori.\r\n\r\n\r\n\r\nBranscombe, N. R., Schmitt, M. T., & Harvey, R. D. (1999). Perceiving pervasive discrimination among african americans: Implications for group identification and well-being. Journal of Personality and Social Psychology, 77(1), 135–149. https://doi.org/10.1037/0022-3514.77.1.135\r\n\r\n\r\nChávez, N. R., & French, S. E. (2007). Ethnicity‐related stressors and mental health in latino americans: The moderating role of parental racial socialization. Journal of Applied Social Psychology, 37(9), 1974–1998. https://doi.org/10.1111/j.1559-1816.2007.00246.x\r\n\r\n\r\nDurie, M. (1998). Whaiora: Māori health development (2nd ed.). Oxford University Press.\r\n\r\n\r\nDurie, M. (2001). Mauri ora: The dynamics of māori health. Oxford University Press.\r\n\r\n\r\nEvans, C. B. R., Smokowski, P. R., & Cotter, K. L. (2014). Individual characteristics, microsystem factors, and proximal relationship processes associated with ethnic identity in rural youth. Journal of the Society for Social Work and Research, 5(1), 45–77. https://doi.org/10.1086/675848\r\n\r\n\r\nHoukamau, C. A., & Sibley, C. G. (2015). The revised multidimensional model of māori identity and cultural engagement (MMM-ICE2). Social Indicators Research, 122(1), 279–296. https://doi.org/10.1007/s11205-014-0686-7\r\n\r\n\r\nMasuoka, N. (2006). Together they become one: Examining the predictors of panethnic group consciousness among asian americans and latinos. Social Science Quarterly, 87(5), 993–1011. https://doi.org/10.1111/j.1540-6237.2006.00412.x\r\n\r\n\r\nMcIntosh, T. (2005). Māori identities: Fixed, fluid, forced. In J. H. Liu, T. McCreanor, T. McIntosh, & T. Teaiwa (Eds.), New zealand identities: Departures and destinations (pp. 67–94). Victoria University Press.\r\n\r\n\r\nPérez, D. J., Fortuna, L., & Alegria, M. (2008). Prevalence and correlates of everyday discrimination among u.s. latinos. Journal of Community Psychology, 36(4), 421–433. https://doi.org/10.1002/jcop.20221\r\n\r\n\r\nRomero, A. J., & Roberts, R. E. (2003). The impact of multiple dimensions of ethnic identity on discrimination and adolescents’ self-esteem. Journal of Applied Social Psychology, 33(11), 2288–2305. https://doi.org/10.1111/j.1559-1816.2003.tb01885.x\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-06T16:08:27+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-25-journal11/",
    "title": "Journal 11",
    "description": "Directed Acyclic Graphs",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-05-25",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-06T14:08:14+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-18-journal10/",
    "title": "Journal 10",
    "description": "Distilling code into webpages",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-05-18",
    "categories": [],
    "contents": "\r\nI have published my Distill website here\r\nAt first I couldn’t get the index webpage to update with changes. I later realised that I needed to update _yml file directly rather than index.html. I’ve found this Distill platform to be very easy to use.\r\nI still don’t quite get the different model equations for varying slopes and intercepts, and the difference between them. I’ll hopefully work on this some more as I feel this is very useful for my intended moderation in my final report.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-05T19:42:40+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-18-workbook10/",
    "title": "Workbook 10",
    "description": "Mixed models: Random slopes and intercepts",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-05-18",
    "categories": [],
    "contents": "\r\nWorkbook 10\r\nHere we will examine a varying intercept and slope model examining the effect of perceived future security on psychological distress, within each generational cohort. We will start by conducting a generalized linear mixed model.\r\n\r\n\r\n#Run linear model with correlated random slope and intercept\r\nmod1 <- lme4::lmer(KESSLER6sum ~ Your.Future.Security + (Your.Future.Security|GenCohort), data=nz_1)\r\nstats1 <- parameters::model_parameters(mod1)\r\n\r\n#Display model table\r\ntab1 <- sjPlot::tab_model(mod1)\r\ntab1\r\n\r\n\r\n\r\n \r\n\r\n\r\nKESSLER6sum\r\n\r\n\r\nPredictors\r\n\r\n\r\nEstimates\r\n\r\n\r\nCI\r\n\r\n\r\np\r\n\r\n\r\n(Intercept)\r\n\r\n\r\n8.86\r\n\r\n\r\n6.40 – 11.32\r\n\r\n\r\n<0.001\r\n\r\n\r\nYour.Future.Security\r\n\r\n\r\n-0.55\r\n\r\n\r\n-0.71 – -0.40\r\n\r\n\r\n<0.001\r\n\r\n\r\nRandom Effects\r\n\r\n\r\nσ2\r\n\r\n12.12\r\n\r\n\r\nτ00GenCohort\r\n\r\n7.66\r\n\r\n\r\nτ11GenCohort.Your.Future.Security\r\n\r\n0.03\r\n\r\n\r\nρ01GenCohort\r\n\r\n-0.98\r\n\r\n\r\nICC\r\n\r\n\r\n0.20\r\n\r\n\r\nN GenCohort\r\n\r\n5\r\n\r\n\r\nObservations\r\n\r\n\r\n21105\r\n\r\n\r\nMarginal R2 / Conditional R2\r\n\r\n0.100 / 0.281\r\n\r\n\r\nHere, the linear model has an intercept of 8.86 which indicates the expected psychological distress score when perceived future security is set to zero.\r\nPerceived future security is an important predictor of psychological distress (beta = -0.55, 95% CI [-0.71, -0.4], p <0.001). Those with higher perceived future security report less psychological distress, which intuitively seems appropriate.\r\nInterestingly, the conditional R^2 value is higher than the marginal R^2 which indicates that the variance explained by the combination of fixed and random effects is greater than that explained by the fixed effects alone. Therefore, the random effect of generational cohort is an important factor to consider. We will examine this further by graphing the data.\r\n\r\n\r\n#Plot model\r\nplot1 <- ggplot2::ggplot(data = nz_1,\r\n                aes(x = Your.Future.Security,\r\n                    y = KESSLER6sum,\r\n                    colour = GenCohort\r\n                    ), na.rm = TRUE) +\r\n  geom_point() +\r\n  geom_smooth(method = \"lm\")\r\nplot1\r\n\r\n\r\n\r\n\r\nThis graph shows a clear difference in the intercepts between generation cohorts. Each successive generation appears to have a greater level of psychological distress than the one previous, when future security is set to zero. We can also see that the rate of change (i.e. slope) between psychological distress and future security seems to be steeper for Gen Z-ers and shallower for members of the Silent Generation, compared to the generations in-between. Stereotypes abound regarding the silent generation’s stoicism versus Zoomers’ emotional fragility/reactivity. Of course, this model does not take into account other factors which may be confounding this relationship such as material conditions of hardship; age and maturity; income, accumulated wealth and job security; each of which may be decreasing across generations.\r\nFor fun, we will also run a Bayesian model of the same parameters, with no priors.\r\n\r\n\r\n#Run Bayesian model with correlated random slope and intercept\r\n#mod2 <- brm(KESSLER6sum ~ Your.Future.Security + (Your.Future.Security|GenCohort),\r\n#            file = here::here(\"data\", \"multilevel-var-slopes-no-prior\"),\r\n#            data = nz_1,\r\n#            family = gaussian\r\n#            )\r\nmod2 <- readRDS(\"multilevel-var-slopes-no-prior.rds\")\r\n\r\n#Display model table\r\ntab2 <- sjPlot::tab_model(mod2)\r\ntab2\r\n\r\n\r\n\r\n \r\n\r\n\r\nKESSLER6sum\r\n\r\n\r\nPredictors\r\n\r\n\r\nEstimates\r\n\r\n\r\nCI (95%)\r\n\r\n\r\nIntercept\r\n\r\n\r\n8.45\r\n\r\n\r\n6.57 – 10.19\r\n\r\n\r\nYour.Future.Security\r\n\r\n\r\n-0.51\r\n\r\n\r\n-0.53 – -0.49\r\n\r\n\r\nRandom Effects\r\n\r\n\r\nσ2\r\n\r\n12.15\r\n\r\n\r\nτ00GenCohort\r\n\r\n3.97\r\n\r\n\r\nICC\r\n\r\n\r\n0.25\r\n\r\n\r\nN GenCohort\r\n\r\n5\r\n\r\n\r\nObservations\r\n\r\n\r\n21105\r\n\r\n\r\nMarginal R2 / Conditional R2\r\n\r\n0.099 / 0.161\r\n\r\n\r\nThe results are quite similar.\r\n\r\n\r\n#Plot model\r\nplot2 <- brms::mcmc_plot(mod2, \r\n               type = \"areas\",\r\n               prob = .89)\r\nplot2\r\n\r\n\r\n\r\n\r\n#Plot model\r\nplot3 <- bayesplot::mcmc_intervals(mod2, \r\n              regex_pars  = c(\"r_\",\"b_\"))\r\nplot3\r\n\r\n\r\n\r\n\r\nThere is group-level variability in psychological distress, as shown by the gradient in scores. We can generate expectation plots for the groups to further explain the relationship.\r\n\r\n\r\n#Calculate uncertainty of the posterior distribution\r\npp <- posterior_predict(mod2)\r\n\r\n#Plot predicted values with posterior distribution\r\nplot4 <- plot(\r\n  ggeffects::ggpredict(mod2, \r\n                       terms = c(\"Your.Future.Security\",\"GenCohort\"),\r\n                       type = \"random\"),\r\n  add.data = pp\r\n)\r\nplot4\r\n\r\n\r\n\r\n\r\nAs previously suggested, perceived future security predicts less psychological distress but successive generations have increased distress.\r\n\r\n\r\n\r\n",
<<<<<<< HEAD
=======
<<<<<<< HEAD
=======
<<<<<<< HEAD
>>>>>>> 1a7c5f4c8e01bcf1fb26d0dfe1a7e734c39fe395
    "preview": {},
    "last_modified": "2021-06-06T16:06:50+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-11-journal9/",
    "title": "Journal 9",
    "description": "Multi-level modelling",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-05-11",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-06T14:10:47+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-11-workbook9/",
    "title": "Workbook 9",
    "description": "Ordinal, cluster, multivariate and mediation models",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-05-11",
    "categories": [],
    "contents": "\r\nHere we will estimate a range of demographic and ideological predictors of attitudes to government surveillance and attitudes to regulate AI in New Zealand in 2018. The particular predictors we have selected include gender, political orientation and patriotism. We will begin by running the Bayesian multivariate model.\r\n\r\n\r\n#Run Bayesian multivariate regression model\r\n#brm(\r\n#  mvbind(Issue.GovtSurveillance, Issue.RegulateAI) ~ Gender + Pol.Orient + PATRIOT,\r\n#  data = nz2018,\r\n#  file = here::here(\"data\", \"multivariate_issues\"), \r\n#)\r\nmod1 <- readRDS(\"multivariate_issues.rds\")\r\nsummary(mod1)\r\n\r\n\r\n Family: MV(gaussian, gaussian) \r\n  Links: mu = identity; sigma = identity\r\n         mu = identity; sigma = identity \r\nFormula: Issue.GovtSurveillance ~ Gender + Pol.Orient + PATRIOT \r\n         Issue.RegulateAI ~ Gender + Pol.Orient + PATRIOT \r\n   Data: nz2018 (Number of observations: 5508) \r\nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\r\n         total post-warmup samples = 4000\r\n\r\nPopulation-Level Effects: \r\n                                 Estimate Est.Error l-95% CI u-95% CI\r\nIssueGovtSurveillance_Intercept      1.58      0.14     1.30     1.85\r\nIssueRegulateAI_Intercept            3.87      0.13     3.62     4.12\r\nIssueGovtSurveillance_Gender        -0.07      0.05    -0.16     0.03\r\nIssueGovtSurveillance_Pol.Orient     0.29      0.02     0.25     0.32\r\nIssueGovtSurveillance_PATRIOT        0.31      0.02     0.26     0.35\r\nIssueRegulateAI_Gender              -0.51      0.05    -0.60    -0.41\r\nIssueRegulateAI_Pol.Orient           0.07      0.02     0.04     0.10\r\nIssueRegulateAI_PATRIOT              0.08      0.02     0.04     0.12\r\n                                 Rhat Bulk_ESS Tail_ESS\r\nIssueGovtSurveillance_Intercept  1.00     8053     3009\r\nIssueRegulateAI_Intercept        1.00     7366     3338\r\nIssueGovtSurveillance_Gender     1.00     8562     3103\r\nIssueGovtSurveillance_Pol.Orient 1.00     7704     2665\r\nIssueGovtSurveillance_PATRIOT    1.00     8959     3293\r\nIssueRegulateAI_Gender           1.00     8651     2779\r\nIssueRegulateAI_Pol.Orient       1.00     8112     3073\r\nIssueRegulateAI_PATRIOT          1.00     7153     2754\r\n\r\nFamily Specific Parameters: \r\n                            Estimate Est.Error l-95% CI u-95% CI Rhat\r\nsigma_IssueGovtSurveillance     1.71      0.02     1.68     1.74 1.00\r\nsigma_IssueRegulateAI           1.61      0.02     1.58     1.64 1.00\r\n                            Bulk_ESS Tail_ESS\r\nsigma_IssueGovtSurveillance     7954     3204\r\nsigma_IssueRegulateAI           8575     2863\r\n\r\nResidual Correlations: \r\n                                              Estimate Est.Error\r\nrescor(IssueGovtSurveillance,IssueRegulateAI)    -0.05      0.01\r\n                                              l-95% CI u-95% CI Rhat\r\nrescor(IssueGovtSurveillance,IssueRegulateAI)    -0.08    -0.03 1.00\r\n                                              Bulk_ESS Tail_ESS\r\nrescor(IssueGovtSurveillance,IssueRegulateAI)     6350     2983\r\n\r\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\n#Plot model\r\nbrms::mcmc_plot(mod1,\r\n               type = \"areas\",\r\n               prob = .89)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-11-workbook9/workbook9_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-06-06T16:35:27+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-04-journal8/",
    "title": "Journal 8",
    "description": "Generalised linear models",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-05-04",
    "categories": [],
    "contents": "\r\nI have found the last few weeks’ workbooks to be more challenging to complete, as there is less specificity around what steps to take to produce models and interpret their fit. I have felt quite lost as to what analysis steps I need to take. I would prefer that there were some questions outlining specific variables of interest, what models to test, and what types of plots are useful to examine. Then, perhaps there could be one question where the analysis is free choice. This would reduce uncertainty about what to include in the workbook answers.\r\nThis was the first time I have produced Bayesian regression models. I was unable to compare the mdoel fit for a Poisson general linear model, and a Bayesian zero-inflated Poisson regression model. I was also unsure how to report on the output of this analysis, despite the lecture section on reporting. Although I have been keeping up in previous lectures, it felt like there was significantly more content to learn this week than before.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-05T19:40:37+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-04-workbook8/",
    "title": "Workbook 8",
    "description": "Generalised linear models",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-05-04",
    "categories": [],
    "contents": "\r\n\r\n\r\n#Centre and scale continuous predictors\r\nnz_1['Household.INC_s'] <- as.data.frame(scale(nz_1$Household.INC))\r\nnz_1['Age'] <- as.data.frame(scale(nz_1$Age))\r\n\r\n#Generate logistic regression model\r\nmod1 <- glm(Believe.Spirit ~ Gender + Age + Household.INC_s,\r\n            data = nz_1,\r\n            family = \"binomial\")\r\n\r\n#Plot model output\r\nstats1 <- parameters::model_parameters(mod1)\r\nprint_md(stats1)\r\n\r\n\r\nParameter\r\nLog-Odds\r\nSE\r\n95% CI\r\nz\r\np\r\n(Intercept)\r\n-1.06\r\n0.06\r\n(-1.19, -0.95)\r\n-17.44\r\n< .001\r\nGender\r\n0.75\r\n0.09\r\n(0.57, 0.93)\r\n8.01\r\n< .001\r\nAge\r\n-0.08\r\n0.05\r\n(-0.18, 0.01)\r\n-1.74\r\n0.083\r\nHousehold.INC_s\r\n0.21\r\n0.05\r\n(0.12, 0.31)\r\n4.49\r\n< .001\r\n\r\nplot(stats1)\r\n\r\n\r\n\r\n#Write model equation with coefficients\r\neq1 <- equatiomatic::extract_eq(mod1, use_coefs = TRUE)\r\nint1 <- plogis(coef(mod1)[[1]])\r\n\r\n#Plot predicted values\r\nplot(ggeffects::ggpredict(mod1, terms = c(\"Gender\", \"Age\", \"Household.INC_s\")), add.data = TRUE, alpha =.1) +\r\n  scale_y_continuous(limits = c(0, 1)) +\r\n  scale_x_continuous(limits = c(0, 1))\r\n\r\n\r\n\r\n#Check model fit\r\ncheck1 <- performance::check_model(mod1)\r\ncheck1\r\n\r\n\r\n\r\n\r\nWe fitted a logistic model (estimated using ML) to predict belief in spirit or a life-force with gender, age and household income. Here, the linear model \\[\r\n\\log\\left[ \\frac { \\widehat{P( \\operatorname{Believe.Spirit} = \\operatorname{Not\\ Believe\\ Spirit} )} }{ 1 - \\widehat{P( \\operatorname{Believe.Spirit} = \\operatorname{Not\\ Believe\\ Spirit} )} } \\right] = -1.06 + 0.75(\\operatorname{Gender}) - 0.08(\\operatorname{Age}) + 0.21(\\operatorname{Household.INC\\_s})\r\n\\] has an intercept of -1.06 which, when used to examine the cumulative density of the logistic distribution, gives an 25.6% probability that males of intermediate age and income believe in spirits or a life-force.\r\nGender is an important predictor of spirit belief (beta = 0.75, 95% CI [0.57, 0.93], p <0.001). Women are more likely to believe in spirits than men, even when accounting for age and household income. Alongside possible biological factors, there may be an effect of gender socialisation at play that lead to greater religious thinking among women.\r\nAge is also an important predictor of spirit belief (beta = -0.08, 95% CI [-0.18, 0.01], p 0.083). Older people are more likely to believe in spirits than younger people, even when accounting for gender and household income. This could potentially be a result of a generational effect e.g. decreasing religiosity among younger generations, or an individual effect e.g. people develop beliefs in spirits as they get older, perhaps in response to growing mortality.\r\nFinally, household income is an important predictor of spirit belief (beta = 0.21, 95% CI [0.12, 0.31], p <0.001). Those living in households with higher income are more likely to believe in spirits than those in households with lower income, even when accounting for gender and age. Spirituality may impact on behaviours that are linked to higher income earners such as religion-based work ethic and material reward.\r\n\r\n\r\n#Generate poisson regression model\r\nmod2 <- glm(CharityDonate ~ Gender + Age + Household.INC_s,\r\n            data = nz_1,\r\n            family = \"poisson\")\r\n\r\n#Plot model output\r\nstats2 <- parameters::model_parameters(mod2)\r\nprint_md(stats2)\r\n\r\n\r\nParameter\r\nLog-Mean\r\nSE\r\n95% CI\r\nz\r\np\r\n(Intercept)\r\n6.84\r\n8.58e-04\r\n(6.83, 6.84)\r\n7971.34\r\n< .001\r\nGender\r\n-0.02\r\n1.38e-03\r\n(-0.02, -0.01)\r\n-12.16\r\n< .001\r\nAge\r\n0.17\r\n7.09e-04\r\n(0.17, 0.17)\r\n240.12\r\n< .001\r\nHousehold.INC_s\r\n0.31\r\n2.59e-04\r\n(0.31, 0.31)\r\n1198.79\r\n< .001\r\n\r\nplot(stats2)\r\n\r\n\r\n\r\n#Write model equation with coefficients\r\neq2 <- equatiomatic::extract_eq(mod2, use_coefs = TRUE)\r\n\r\n#Plot predicted values\r\nplot(ggeffects::ggpredict(mod2, terms = c(\"Gender\", \"Age\", \"Household.INC_s\")), add.data = TRUE, alpha =.1) +\r\n  scale_y_continuous(limits = c(0, 1)) +\r\n  scale_x_continuous(limits = c(0, 1))\r\n\r\n\r\n\r\n#Check model fit\r\ncheck2 <- performance::check_model(mod2)\r\ncheck2\r\n\r\n\r\n\r\n#Check zero inflation and overdispersion\r\ncheck3 <- check_zeroinflation(mod2)\r\ncheck3\r\n\r\n\r\n# Check for zero-inflation\r\n\r\n   Observed zeros: 330\r\n  Predicted zeros: 0\r\n            Ratio: 0.00\r\n\r\ncheck4 <- check_overdispersion(mod2)\r\ncheck4\r\n\r\n\r\n# Overdispersion test\r\n\r\n       dispersion ratio =    13589.080\r\n  Pearson's Chi-Squared = 31499488.564\r\n                p-value =      < 0.001\r\n\r\nNext, we fitted a poisson model (estimated using ML) to predict charitable donations with gender, age and household income. Here, the linear model \\[\r\n\\log ({ \\widehat{E( \\operatorname{CharityDonate} )} })  = 6.84 - 0.02(\\operatorname{Gender}) + 0.17(\\operatorname{Age}) + 0.31(\\operatorname{Household.INC\\_s})\r\n\\] has an intercept of 6.84 which indicates the log of the expected donation amount in the last year ($930.27) for males of intermediate age and income.\r\nGender is an important predictor of charitable donations (beta = -0.02, 95% CI [-0.02, -0.01], p <0.001). Women donate more than men, even when accounting for age and household income. This may reflect gender socialisation factors such as an expectation to care for others.\r\nAge is also an important predictor of charitable donations (beta = 0.17, 95% CI [0.17, 0.17], p <0.001). Older people donate more than younger people, even when accounting for gender and household income. This could potentially be a result of a generational effect e.g. decreasing generosity to strangers among younger generations, or an individual effect e.g. increased discretionary income among older people.\r\nFinally, household income is an important predictor of charitable donations (beta = 0.31, 95% CI [0.31, 0.31], p <0.001). Those living in households with higher income donate more than those in households with lower income, even when accounting for gender and age. This seems to be fairly self-explpanatory, with higher income households having greater discretionary income to spend on charitable donations.\r\nChecking our model fit, we find that there is evidence of zero-inflation and overdispersion in the model. We will therefore compare the Poisson regression model with a zero-inflated Poisson regression model to see which is the better fit.\r\n\r\n\r\n#Set Charity Donation variable to integer\r\nnz_1$CharityDonate <- as.integer(nz_1$CharityDonate)\r\n\r\n#Generate zero-inflated poisson regression model\r\n#brms::brm(CharityDonate ~ Gender + Age + Household.INC_s,\r\n#                   family = \"zero_inflated_poisson\",\r\n#                   file = here::here(\"data\", \"zeroinflated_poisson_donate\"),\r\n#                   data = nz_1)\r\nbmod1 <- readRDS(\"zeroinflated_poisson_donate.rds\")\r\nstats3 <- summary(bmod1)\r\nstats3\r\n\r\n\r\n Family: zero_inflated_poisson \r\n  Links: mu = log; zi = identity \r\nFormula: CharityDonate ~ Gender + Age + Household.INC_s \r\n   Data: nz_1 (Number of observations: 5329) \r\nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\r\n         total post-warmup samples = 4000\r\n\r\nPopulation-Level Effects: \r\n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\r\nIntercept           6.75      0.00     6.75     6.76 1.00     3145\r\nGender              0.70      0.00     0.70     0.70 1.00     2675\r\nAge                 0.31      0.00     0.31     0.31 1.00     3918\r\nHousehold.INC_s     0.23      0.00     0.23     0.23 1.00     4371\r\n                Tail_ESS\r\nIntercept           2850\r\nGender              2401\r\nAge                 2929\r\nHousehold.INC_s     3222\r\n\r\nFamily Specific Parameters: \r\n   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nzi     0.14      0.00     0.13     0.15 1.01      639      646\r\n\r\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\n#Plot predicted values from posterior samples\r\nplot(\r\n  conditional_effects(\r\n    bmod1,\r\n    spaghetti = TRUE,\r\n    nsamples = 100,\r\n    select_points = 0.1\r\n  ),\r\n  points = TRUE,\r\n  point_args = list(alpha = 0.1,\r\n                    width = .02)\r\n)\r\n\r\n\r\n\r\n#Zero-inflated poisson model posterior predictive check\r\nbrms::pp_check(bmod1) + xlim(0, 5)\r\n\r\n\r\n\r\n#Compare models\r\n#b0 <- add_criterion(mod2, \"loo\")\r\n#b1 <- add_criterion(bmod1, \"loo\")\r\n#w <- loo_compare(b0, b1, criterion = \"loo\")\r\n#w\r\n#cbind(waic_diff = w[, 1] * -2,\r\n#      se        = w[, 2] * 2)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-04-workbook8/workbook8_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-06-06T16:22:07+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-27-journal7/",
    "title": "Journal 7",
    "description": "Other types of modelling",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-04-27",
    "categories": [],
    "contents": "\r\nThis week covered a whole range of statistical models, including ordinal responses and predictors, random intercept models, multivariate models and mediation.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-06T14:07:59+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-27-workbook7/",
    "title": "Workbook 7",
    "description": "Multiple regression",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-04-27",
    "categories": [],
    "contents": "\r\nI have chosen to examine five variables relating to psychological distress and micro-/macro-level economic security variables from Wave 11 (2019) of the NZ Attitudes and Values Survey (NZAVS).\r\n\r\n\r\n#Create tibble of variable means and standard deviations\r\nstats1 <- nz_1 %>%\r\n  select(KESSLER6sum,\r\n         NZ.Business.Conditions,\r\n         NZ.Economic.Situation,\r\n         Your.Future.Security,\r\n         Emp.JobSecure\r\n         ) %>%\r\n  summarise(\r\n    round(\r\n      rbind(\r\n        across(c(KESSLER6sum,\r\n                 NZ.Business.Conditions,\r\n                 NZ.Economic.Situation,\r\n                 Your.Future.Security,\r\n                 Emp.JobSecure\r\n                 ), ~mean(.x, na.rm = TRUE)\r\n               ),\r\n        across(c(KESSLER6sum,\r\n                 NZ.Business.Conditions,\r\n                 NZ.Economic.Situation,\r\n                 Your.Future.Security,\r\n                 Emp.JobSecure\r\n                 ), ~sd(.x, na.rm = TRUE)\r\n               )\r\n        ), digits = 2)\r\n    )\r\n\r\n#Create table to store variable means and standard deviations\r\ntab1 <- t(stats1) %>%\r\n  kbl(caption = \"Psychological distress and economic security variables in the NZAVS 2019 wave\",\r\n      col.names = c(\"M\",\r\n                    \"SD\"),\r\n      align = c(\"r\", \"r\")\r\n      ) %>%\r\n  kable_classic_2(c(\"striped\", \"hover\"), full_width = TRUE)\r\ntab1\r\n\r\n\r\n\r\nTable 1: Psychological distress and economic security variables in the NZAVS 2019 wave\r\n\r\n\r\n\r\n\r\nM\r\n\r\n\r\nSD\r\n\r\n\r\nKESSLER6sum\r\n\r\n\r\n4.73\r\n\r\n\r\n3.81\r\n\r\n\r\nNZ.Business.Conditions\r\n\r\n\r\n5.71\r\n\r\n\r\n1.88\r\n\r\n\r\nNZ.Economic.Situation\r\n\r\n\r\n5.56\r\n\r\n\r\n2.15\r\n\r\n\r\nYour.Future.Security\r\n\r\n\r\n6.54\r\n\r\n\r\n2.23\r\n\r\n\r\nEmp.JobSecure\r\n\r\n\r\n5.45\r\n\r\n\r\n1.56\r\n\r\n\r\n#Create density plots of variables\r\nnz_2 <- nz_1 %>%\r\n  select(KESSLER6sum,\r\n         NZ.Business.Conditions,\r\n         NZ.Economic.Situation,\r\n         Your.Future.Security,\r\n         Emp.JobSecure\r\n         ) %>%\r\n  melt()\r\nplot1 <- ggplot(data = nz_2, aes(x = value, color = variable, fill = variable)) + \r\n  geom_density(alpha = 0.7) +\r\n  facet_wrap(~ variable, nrow = 2) +\r\n  labs(title = str_wrap(\"Density plots of psychological distress and economic security variables in NZAVS 2019 wave\", 60)) +\r\n  ylab(\"Density\") +\r\n  xlab(\"Variable\") +\r\n  theme(plot.title = element_text(size=14),\r\n        axis.text.x = element_text(angle = 20, hjust = 1),\r\n        legend.position = \"none\"\r\n  )\r\nplot1\r\n\r\n\r\n\r\n#Create correlation plot of variables\r\nextrafont::loadfonts()\r\nplot2 <- nz_1 %>%\r\n  select(KESSLER6sum,\r\n         NZ.Business.Conditions,\r\n         NZ.Economic.Situation,\r\n         Your.Future.Security,\r\n         Emp.JobSecure\r\n         ) %>%\r\n  correlation(partial = FALSE, multilevel = FALSE) %>%\r\n  plot()\r\nplot2\r\n\r\n\r\n\r\n\r\nPsychological distress was measured using the Kessler 6. The six items contribute to a total possible score of 24, however the average amongst the general NZ population is relatively low (M = 4.73, SD = 3.81). The density plot shows a platykurtic distribution with positive skew, indicating the bulk of scores were low but with a few high-scoring individuals.\r\nPersonal and national economic wellbeing factors were measured on a scale from 0 (“Completely Dissatisfied”) to 10 (“Completely Satisfied”). Satisfaction with NZ business conditions (M = 5.71, SD = 1.88), the NZ economy (M = 5.56, SD = 2.15) and perceived future security (M = 6.54, SD = 2.23) were relatively mid-range. Density plots show reasonably similar distributions.\r\nPerceived job security was measured on a scale from 1 (“Not secure”) to 7 (“Very secure”) with slightly higher average responses (M = 5.45, SD = 1.56). The density plot shows a leptokurtic distribution with negative skew.\r\nA correlation plot shows that all four economic security variables are positively correlated with each other, with perceived job security having the weakest correlation. Psychological distress was negatively correlated with the four economic security variables indicating that as perceived economic security increases, distress decreases.\r\n\r\n\r\n#Run linear model with one predictor\r\nmod1 <- lm(KESSLER6sum ~ NZ.Economic.Situation.c, data = nz_1)\r\ntab2 <- parameters::model_parameters(mod1)\r\nprint_md(tab2)\r\n\r\n\r\nParameter\r\nCoefficient\r\nSE\r\n95% CI\r\nt(2058)\r\np\r\n(Intercept)\r\n4.71\r\n0.08\r\n(4.55, 4.87)\r\n57.37\r\n< .001\r\nNZ.Economic.Situation.c\r\n-0.34\r\n0.04\r\n(-0.41, -0.26)\r\n-8.81\r\n< .001\r\n\r\n#Plot linear model\r\nplot3 <- plot(ggeffects::ggpredict(mod1,\r\n                                   terms = \"NZ.Economic.Situation.c\"\r\n                                   ),\r\n              add.data = TRUE,\r\n              jitter = 0.2,\r\n              dot.alpha =.2,\r\n              ) +\r\n  scale_x_continuous(limits = c(-5, 5)) +\r\n  scale_y_continuous(limits = c(0, 7)) +\r\n  theme_classic()\r\nplot3\r\n\r\n\r\n\r\n#Write model equation with coefficients\r\neq1 <- equatiomatic::extract_eq(mod1,  use_coefs = TRUE)\r\neq1\r\n\r\n\r\n\\[\r\n\\operatorname{\\widehat{KESSLER6sum}} = 4.71 - 0.34(\\operatorname{NZ.Economic.Situation.c})\r\n\\]\r\n\r\nHere, the linear model \\[\r\n\\operatorname{\\widehat{KESSLER6sum}} = 4.71 - 0.34(\\operatorname{NZ.Economic.Situation.c})\r\n\\] has an intercept of 4.71 which indicates the expected score when satisfaction with the NZ economy is the average score.\r\nSatisfaction with the NZ economy is an important predictor of psychological distress (beta = -0.34, 95% CI = 0.95, p <0.001).\r\n\r\n\r\n#Run linear model with two predictors\r\nmod2 <- lm(KESSLER6sum ~ NZ.Economic.Situation.c + Your.Future.Security.c, data = nz_1)\r\ntab3 <- parameters::model_parameters(mod2)\r\nprint_md(tab3)\r\n\r\n\r\nParameter\r\nCoefficient\r\nSE\r\n95% CI\r\nt(2054)\r\np\r\n(Intercept)\r\n4.70\r\n0.08\r\n(4.55, 4.85)\r\n60.68\r\n< .001\r\nNZ.Economic.Situation.c\r\n-0.09\r\n0.04\r\n(-0.17, -0.01)\r\n-2.29\r\n0.022\r\nYour.Future.Security.c\r\n-0.60\r\n0.04\r\n(-0.68, -0.53)\r\n-16.04\r\n< .001\r\n\r\n#Plot linear model\r\nplot4 <- plot(ggeffects::ggpredict(mod2,\r\n                                   terms = c(\"NZ.Economic.Situation.c\",\r\n                                             \"Your.Future.Security.c\")\r\n                                   ),\r\n              add.data = TRUE,\r\n              jitter = 0.2,\r\n              dot.alpha =.2,\r\n              ) +\r\n  scale_x_continuous(limits = c(-5, 5)) +\r\n  scale_y_continuous(limits = c(0, 7)) +\r\n  theme_classic()\r\nplot4\r\n\r\n\r\n\r\n#Write model equation with coefficients\r\neq2 <- equatiomatic::extract_eq(mod2,  use_coefs = TRUE)\r\neq2\r\n\r\n\r\n\\[\r\n\\operatorname{\\widehat{KESSLER6sum}} = 4.7 - 0.09(\\operatorname{NZ.Economic.Situation.c}) - 0.6(\\operatorname{Your.Future.Security.c})\r\n\\]\r\n\r\nHere, the linear model \\[\r\n\\operatorname{\\widehat{KESSLER6sum}} = 4.7 - 0.09(\\operatorname{NZ.Economic.Situation.c}) - 0.6(\\operatorname{Your.Future.Security.c})\r\n\\] has an intercept of 4.7 which indicates the expected score when satisfaction with the NZ economy is the average score.\r\nSatisfaction with the NZ economy is no longer an important predictor of psychological distress (beta = -0.09, 95% CI = 0.95, p <0.001) as its explanatory power is obscured by perceived future security (beta = -0.6, 95% CI = 0.95, p <0.001).\r\n\r\n\r\n#Compare models\r\nmodcomp <- performance::compare_performance(mod1,mod2)\r\nmodcomp\r\n\r\n\r\n# Comparison of Model Performance Indices\r\n\r\nName | Model |       AIC |       BIC |    R2 | R2 (adj.) |  RMSE | Sigma\r\n------------------------------------------------------------------------\r\nmod1 |    lm | 11272.048 | 11288.939 | 0.036 |     0.036 | 3.727 | 3.729\r\nmod2 |    lm | 11013.321 | 11035.837 | 0.143 |     0.143 | 3.512 | 3.514\r\n\r\nplot(modcomp)\r\n\r\n\r\n\r\n\r\nComparing the two models, the second model with two predictors appears to be the better fit for the data. AIC, BIC and RMSE both decreased for the second model, while the R^2 value increased. Inclusion of perceived future security is therefore a better predictor of psychological distress than satisfaction with the NZ economy alone.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-27-workbook7/workbook7_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-06-06T16:21:04+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-20-journal6/",
    "title": "Journal 6",
    "description": "Reporting success",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-04-20",
    "categories": [],
    "contents": "\r\nI opted to write my report introduction using Papaja. I was largely successful at using the citation skills from the first assignment, however I was unable to seperate out the R package references from appearing in the reference list. I am unsure whether it is accepted APA practice to include technical references i.e. programmes in a separate list. For this reason it looks slightly messy!\r\nI would like to better understand how to integrate tables and graphs within Papaja. Some of the preliminary data findings could have been better represented in this way. However, I had difficulty using the Kbl() function as I had in my previous workbook. There must be Papaja-specific functions that enable these to work.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-05T19:38:10+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-20-workbook6/",
    "title": "Workbook 6",
    "description": "Prototypicality, discrimination and Maori identity",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-04-20",
    "categories": [],
    "contents": "\r\nDoes prototypicality of group membership moderate the effect of intergroup and intragroup discrimination on ethnic identity in Māori?\r\nMāori culture has been posited (e.g. by Durie, 1998, 2001) as a protective factor for psychological wellbeing. Strengthening cultural/ethnic identity may be a means to addessing disparities in health, education and other contexts. However, not all people who identify as having whakapapa Māori (Māori ancestry) have the access, means or desire to adopt a traditional, “fixed” Māori identity (McIntosh, 2005). In this project I will examine the heterogeneity of Māori cultural/ethnic identity formation through participants’ experiences of discrimination by Pākehā/Tauiwi and Māori.\r\nBranscombe, Schmitt and Harvey’s (1999) rejection-identification model proposes that perceived discrimination by the dominant group leads to strengthened ingroup identification, as a defense mechanism to protect the individual’s psychological wellbeing. Evidence for this model is mixed with some studies showing increased ethnic identity while others do not (Chávez & French, 2007; Evans et al., 2014; Masuoka, 2006; Pérez et al., 2008; Romero & Roberts, 2003). One explanation for these mixed results may be that some individuals are more or less able to traverse boundaries of group category membership because of how prototypical they appear or behave. Evidence suggests that self-rated similarity to the average ingroup member mediates the relationship between perceived discrimination and life satisfaction [giamo-2012]. However, research to date has not examined the effect of group membership prototypicality on the relationship between perceived discrimination and ethnic identity. If highly prototypical group members are less able than low prototypicality members to distance themselves from the group category for which they are being targeted for discrimination, then they may have more reason to defend whatever limited identity choices are available to them.\r\nI want to explore whether the diversity of participants’ self-rated prototypicality to the group category ‘Māori’ based on physical, cultural and social characteristics affects how discrimination experiences are rejected or internalised through strengthened or decreased identification as Māori. The main research questions are:\r\nDoes prototypicality of group membership moderate the effect of perceived discrimination by Pākehā/Tauiwi, on Māori identity?\r\nDoes prototypicality of group membership moderate the effect of perceived discrimination by Māori, on Māori identity?\r\nMethod\r\nParticipants\r\nParticipants were 669 members of the community who identified as Māori (62.9% female, 37.1% male; Meanage=44.4, SDage=13) recruited through the New Zealand Attitudes and Values Survey.\r\nMaterial\r\nParticipants completed a range of demographic questions and construct scales. The constructs used in the current study are included below.\r\nPerceived discrimination. Participants were asked to evaluate the single item, “I feel that I am often discriminated against because of my ethnicity” on a scale from 1 (Strongly Disagree) to 7 (Strongly Agree).\r\nPerceived appearance. This subscale of Houkamau and Sibley’s (2015) MMM-ICE2 examines self-rated prototypicality of physical appearance to the Māori ethnic group. Seven items including “I think it is easy to tell that I am Māori just by looking at me” and “People would never know that I am of Māori descent just by looking at me” are rated on a scale from 1 (Strongly Disagree) to 7 (Strongly Agree).\r\nGroup membership and evaluation. Another subscale of the MMM-ICE2 examines affective and centrality components of ethnic group membership (i.e. identity). Eight items including “I love the fact I am Māori” and “My Māori ancestry is important to me” are rated on a scale from 1 (Strongly Disagree) to 7 (Strongly Agree).\r\nProcedure\r\nParticipants were posted a copy of the questionnaire, with a second postal follow-up two months later. Participants who provided an email address were also emailed and invited to complete an online version if they preferred.\r\nTo boost sample size and increase sample diversity for subsequent waves, booster samples using different sample frames were conducted. Booster sampling was conducted without replacement (i.e., all people included in previous sample frames were identified and removed from the electoral roll before generation of the new sample frames).\r\nThe fifth sample frame consisted of 9,000 people randomly selected from those who indicated on the 2012 Electoral Roll that they were of Māori ethnicit y (ethnic affiliation as Māori is listed on the roll, but other ethnic affiliations are not). A total of 689 participants responded to this booster sample (adjusted response rate = 7.78%). The questionnaire administered to the Māori booster sample included questions specifically designed for Māori.\r\n\r\n\r\n\r\nBranscombe, N. R., Schmitt, M. T., & Harvey, R. D. (1999). Perceiving pervasive discrimination among african americans: Implications for group identification and well-being. Journal of Personality and Social Psychology, 77(1), 135–149. https://doi.org/10.1037/0022-3514.77.1.135\r\n\r\n\r\nChávez, N. R., & French, S. E. (2007). Ethnicity‐related stressors and mental health in latino americans: The moderating role of parental racial socialization. Journal of Applied Social Psychology, 37(9), 1974–1998. https://doi.org/10.1111/j.1559-1816.2007.00246.x\r\n\r\n\r\nDurie, M. (1998). Whaiora: Māori health development (2nd ed.). Oxford University Press.\r\n\r\n\r\nDurie, M. (2001). Mauri ora: The dynamics of māori health. Oxford University Press.\r\n\r\n\r\nEvans, C. B. R., Smokowski, P. R., & Cotter, K. L. (2014). Individual characteristics, microsystem factors, and proximal relationship processes associated with ethnic identity in rural youth. Journal of the Society for Social Work and Research, 5(1), 45–77. https://doi.org/10.1086/675848\r\n\r\n\r\nHoukamau, C. A., & Sibley, C. G. (2015). The revised multidimensional model of māori identity and cultural engagement (MMM-ICE2). Social Indicators Research, 122(1), 279–296. https://doi.org/10.1007/s11205-014-0686-7\r\n\r\n\r\nMasuoka, N. (2006). Together they become one: Examining the predictors of panethnic group consciousness among asian americans and latinos. Social Science Quarterly, 87(5), 993–1011. https://doi.org/10.1111/j.1540-6237.2006.00412.x\r\n\r\n\r\nMcIntosh, T. (2005). Māori identities: Fixed, fluid, forced. In J. H. Liu, T. McCreanor, T. McIntosh, & T. Teaiwa (Eds.), New zealand identities: Departures and destinations (pp. 67–94). Victoria University Press.\r\n\r\n\r\nPérez, D. J., Fortuna, L., & Alegria, M. (2008). Prevalence and correlates of everyday discrimination among u.s. latinos. Journal of Community Psychology, 36(4), 421–433. https://doi.org/10.1002/jcop.20221\r\n\r\n\r\nRomero, A. J., & Roberts, R. E. (2003). The impact of multiple dimensions of ethnic identity on discrimination and adolescents’ self-esteem. Journal of Applied Social Psychology, 33(11), 2288–2305. https://doi.org/10.1111/j.1559-1816.2003.tb01885.x\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-06T15:18:28+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-27-journal5/",
    "title": "Journal 5",
    "description": "Linear models",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-03-27",
    "categories": [],
    "contents": "\r\nI’m quite pleased with how things turned out in this week’s workbook. Although the assessment was intended to be a lighter workload than last week’s, this actually provided more opportunity to try out different code to see what would be more efficient or give tidier output. For example, the instructions to simply print a table and a graph allowed me to test out the kbl() function and its arguments. I did however struggle with formatting the table output to APA standards e.g. italicising column labels.\r\nI also tried using inline call functions to pull statistics directly into my written analysis. I believe this will save a lot of time going forward, as it means I can create template reports before conducting analysis of the dataset.\r\nMy primary concern at this stage is preparation for the final report assessment. I would like to simulate data for my 489 project which is still in the data collection phase. My research question is whether prototypicality of group membership moderates the effect of perceived discrimination on ethnic identity in Māori (i.e. the rejection-identification model). As such, I am not sure how to best simulate data where there are two expected groups of high and low prototypicality participants who may or may not differ in the strength of linear relationship between discrimination and identity. If this is not possible, I would also be open to conducting the same type of moderation analysis on another dataset so that at least I will have the correct code to use for my 489.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-05T19:29:03+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-27-workbook5/",
    "title": "Workbook 5",
    "description": "Linear models",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-03-27",
    "categories": [],
    "contents": "\r\n\r\n\r\n#Read data\r\nnz <- readr::read_delim(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/nz/nz.csv\"),\r\n                        delim = \";\",\r\n                        locale=locale(decimal_mark = \",\"),\r\n                        col_types = cols())\r\n\r\n#Re-level Kessler 6 variables and filter dataset by 2019 survey wave\r\nf<-c(\"None Of The Time\",\r\n     \"A Little Of The Time\",\r\n     \"Some Of The Time\",\r\n     \"Most Of The Time\",\r\n     \"All Of The Time\")\r\nnz_1 <- nz %>%\r\n  dplyr::mutate_if(is.character, factor) %>%\r\n  select(\r\n    -c(\r\n      SWB.Kessler01,\r\n      SWB.Kessler02,\r\n      SWB.Kessler03,\r\n      SWB.Kessler04,\r\n      SWB.Kessler05,\r\n      SWB.Kessler06\r\n    )\r\n  ) %>%\r\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\r\n  mutate(FeelHopeless = forcats::fct_relevel(FeelHopeless, f)) %>%\r\n  mutate(FeelDepressed = forcats::fct_relevel(FeelDepressed, f)) %>%\r\n  mutate(FeelRestless = forcats::fct_relevel(FeelRestless, f)) %>%\r\n  mutate(EverythingIsEffort = forcats::fct_relevel(EverythingIsEffort, f)) %>%\r\n  mutate(FeelWorthless = forcats::fct_relevel(FeelWorthless, f)) %>%\r\n  mutate(FeelNervous = forcats::fct_relevel(FeelNervous, f)) %>%\r\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\r\n  dplyr::mutate(male_id = as.factor(Male)) %>%\r\n  dplyr::mutate(date = make_date(year = 2009, month = 6, day = 30) + TSCORE) %>%\r\n  filter(Wave==\"2019\")\r\n\r\n#Read data\r\nmd_df <- data.frame(read.table(url(\"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/PearsonLee/data/MotherDaughterHeights.txt\"),\r\n                               header=TRUE))\r\n\r\n#Centre mother_height; convert daughter_height and mother_height to metres\r\nmd_df <- md_df %>%\r\n  dplyr::mutate(mother_height_c = as.numeric(scale(mother_height, center = TRUE, scale = FALSE))) %>%\r\n  dplyr::mutate(daughter_height = conv_unit(daughter_height, \"inch\", \"m\")) %>%\r\n  dplyr::mutate(mother_height = conv_unit(mother_height, \"inch\", \"m\"))\r\n\r\n\r\n\r\nQuestion 1\r\n\r\n\r\n#Present means and standard deviations for weight and height in a table\r\nstats1 <- nz_1 %>%\r\n  select(HLTH.Weight, HLTH.Height, male_id) %>%\r\n  filter(!is.na(male_id)) %>%\r\n  summarise(\r\n    \"male_id\" = as.factor(\"Total\"),\r\n    \"m.w\" = round(mean(HLTH.Weight, na.rm = TRUE), digits = 1),\r\n    \"sd.w\" = round(sd(HLTH.Weight, na.rm = TRUE), digits = 1),\r\n    \"m.h\" = round(mean(HLTH.Height, na.rm = TRUE), digits = 2),\r\n    \"sd.h\" = round(sd(HLTH.Height, na.rm = TRUE), digits = 2),\r\n    \"n\" = n()\r\n    )\r\nstats2 <- nz_1 %>%\r\n  select(HLTH.Weight, HLTH.Height, male_id) %>%\r\n  filter(!is.na(male_id)) %>%\r\n  group_by(male_id) %>%\r\n  summarise(\r\n    \"m.w\" = round(mean(HLTH.Weight, na.rm = TRUE), digits = 1),\r\n    \"sd.w\" = round(sd(HLTH.Weight, na.rm = TRUE), digits = 1),\r\n    \"m.h\" = round(mean(HLTH.Height, na.rm = TRUE), digits = 2),\r\n    \"sd.h\" = round(sd(HLTH.Height, na.rm = TRUE), digits = 2),\r\n    \"n\" = n()\r\n    )\r\nstats3 <- rbind(stats1, stats2)\r\ntab1 <- stats3 %>%\r\n  kbl(caption = \"Participants' weights and heights by gender in NZAVS 2019 wave\",\r\n      col.names = c(\"Gender\",\r\n                    \"Weight (M)\",\r\n                    \"Weight (SD)\",\r\n                    \"Height (M)\",\r\n                    \"Height (SD)\",\r\n                    \"n\"),\r\n      align = c(\"r\", \"r\", \"r\", \"r\")\r\n      ) %>%\r\n  kable_classic_2(c(\"striped\", \"hover\"), full_width = TRUE)\r\ntab1\r\n\r\n\r\n\r\nTable 1: Participants’ weights and heights by gender in NZAVS 2019 wave\r\n\r\n\r\nGender\r\n\r\n\r\nWeight (M)\r\n\r\n\r\nWeight (SD)\r\n\r\n\r\nHeight (M)\r\n\r\n\r\nHeight (SD)\r\n\r\n\r\nn\r\n\r\n\r\nTotal\r\n\r\n\r\n79.7\r\n\r\n\r\n18.8\r\n\r\n\r\n1.70\r\n\r\n\r\n0.10\r\n\r\n\r\n2057\r\n\r\n\r\nMale\r\n\r\n\r\n88.2\r\n\r\n\r\n17.1\r\n\r\n\r\n1.78\r\n\r\n\r\n0.08\r\n\r\n\r\n752\r\n\r\n\r\nNot_Male\r\n\r\n\r\n74.7\r\n\r\n\r\n18.0\r\n\r\n\r\n1.65\r\n\r\n\r\n0.07\r\n\r\n\r\n1305\r\n\r\n\r\n#Generate notched boxplots of weight and height\r\nplot1 <- ggplot(nz_1, aes(y = HLTH.Weight)) +\r\n  geom_boxplot(notch = TRUE, na.rm = TRUE) +\r\n  theme_apa() +\r\n  theme(axis.text.x = element_blank(),\r\n        axis.ticks.x = element_blank()\r\n        ) +\r\n  ylab(\"Weight (kg)\")\r\n\r\nplot2 <- ggplot(nz_1, aes(y = HLTH.Height)) +\r\n  geom_boxplot(notch = TRUE, na.rm = TRUE) +\r\n  theme_apa() +\r\n  theme(axis.text.x = element_blank(),\r\n        axis.ticks.x = element_blank()\r\n        ) +\r\n  ylab(\"Height (m)\")\r\n\r\n#Combine boxplots into one figure\r\nplot1 + plot2 + \r\n  plot_annotation(title = \"Participants' weights and heights in NZAVS 2019 wave\",\r\n                  tag_levels = 'a')\r\n\r\n\r\n\r\n\r\nQuestion 2\r\nThe mean weight of participants was 79.7 kilograms (SD = 18.8). Males were heavier than average (M = 88.2, SD = 17.1) while non-males were lighter than average (M = 74.7, SD = 18).\r\nThe mean height of participants was 1.7 metres (SD = 0.1). Males were taller than average (M = 1.78, SD = 0.08) while non-males were lighter than average (M = 1.65, SD = 0.07).\r\nQuestion 3\r\n\r\n\r\n#Run linear model of height by weight in the NZAVS dataset\r\nmod1 <- lm(HLTH.Height ~ HLTH.Weight, data = nz_1)\r\np1 <- scales::pvalue((tidy(mod1)$p.value[2]))\r\nrs1 <- round(summary(mod1)$r.squared, 3)\r\n\r\n#Present results in a table\r\ntab3 <- mod1 %>%\r\n  tidy() %>%\r\n  mutate(p.value = scales::pvalue(p.value),\r\n        term = c(\"Intercept\", \"Weight\")\r\n  ) %>%\r\n  kbl(caption = \"Linear regression of height by weight in NZAVS 2019 wave\",\r\n      col.names = c(\"Predictor\", \"B\", \"SE\", \"t\", \"p\"),\r\n      digits = c(0, 3, 3, 2, 3),\r\n      align = c(\"l\", \"r\", \"r\", \"r\", \"r\")\r\n  ) %>%\r\n  footnote(\r\n    general = c(\"R$^2$ = \", round(summary(mod1)$r.squared, 3)),\r\n    general_title = \"Note.\",\r\n    footnote_as_chunk = TRUE,\r\n    escape = FALSE\r\n  ) %>%\r\n  kable_classic_2(c(\"striped\", \"hover\"), full_width = TRUE)\r\ntab3\r\n\r\n\r\n\r\nTable 2: Linear regression of height by weight in NZAVS 2019 wave\r\n\r\n\r\nPredictor\r\n\r\n\r\nB\r\n\r\n\r\nSE\r\n\r\n\r\nt\r\n\r\n\r\np\r\n\r\n\r\nIntercept\r\n\r\n\r\n1.525\r\n\r\n\r\n0.009\r\n\r\n\r\n175.01\r\n\r\n\r\n<0.001\r\n\r\n\r\nWeight\r\n\r\n\r\n0.002\r\n\r\n\r\n0.000\r\n\r\n\r\n20.32\r\n\r\n\r\n<0.001\r\n\r\n\r\nNote.  R\\(^2\\) =  0.169\r\n\r\n\r\n#Graph results\r\nplot3 <- ggplot(data = nz_1, aes(y = HLTH.Height, x = HLTH.Weight)) + \r\n  geom_jitter(alpha = .2, na.rm = TRUE) + \r\n  geom_smooth(method = \"lm\", se = FALSE, col = \"black\", na.rm = TRUE) +\r\n  labs(title = \"Linear regression of height by weight in NZAVS 2019 wave\") +\r\n  ylab(\"Height\") +\r\n  xlab(\"Weight\") +\r\n  theme_apa()\r\nplot3\r\n\r\n\r\n\r\n\r\nThe null hypothesis that there is no relationship between height and weight is rejected at the 5% significance level (p <0.001). There is strong evidence to suggest that there is a linear relationship between height and weight, with the fitted regression line \\[\r\n\\operatorname{\\widehat{HLTH.Height}} = 1.525 + 0.002(\\operatorname{HLTH.Weight})\r\n\\]\r\nWeight is therefore a useful linear predictor of height, explaining 16.9% (R\\(^2\\) = 0.169) of the variance in the model. The scatterplot with fitted regression line shows that height increases as weight increases, suggesting that physiological development of these factors occurs in tandem.\r\nQuestion 4\r\n\r\n\r\n#Run linear model of height by gender in the NZAVS dataset\r\nmod2 <- lm(HLTH.Height ~ male_id, data = nz_1)\r\np2 <- scales::pvalue((tidy(mod2)$p.value[2]))\r\nrs2 <- round(summary(mod2)$r.squared, 3)\r\n\r\n#Present results in a table\r\ntab4 <- mod2 %>%\r\n  tidy() %>%\r\n  mutate(p.value = scales::pvalue(p.value),\r\n        term = c(\"Intercept\", \"Gender (not male)\")\r\n  ) %>%\r\n  kbl(caption = \"Linear regression of height by gender in NZAVS 2019 wave\",\r\n      col.names = c(\"Predictor\", \"B\", \"SE\", \"t\", \"p\"),\r\n      digits = c(0, 2, 3, 2, 3),\r\n      align = c(\"l\", \"r\", \"r\", \"r\", \"r\")\r\n  ) %>%\r\n  footnote(\r\n    general = c(\"R$^2$ = \", round(summary(mod2)$r.squared, 3)),\r\n    general_title = \"Note.\",\r\n    footnote_as_chunk = TRUE,\r\n    escape = FALSE\r\n  ) %>%\r\n  kable_classic_2(c(\"striped\", \"hover\"), full_width = TRUE)\r\ntab4\r\n\r\n\r\n\r\nTable 3: Linear regression of height by gender in NZAVS 2019 wave\r\n\r\n\r\nPredictor\r\n\r\n\r\nB\r\n\r\n\r\nSE\r\n\r\n\r\nt\r\n\r\n\r\np\r\n\r\n\r\nIntercept\r\n\r\n\r\n1.78\r\n\r\n\r\n0.003\r\n\r\n\r\n628.10\r\n\r\n\r\n<0.001\r\n\r\n\r\nGender (not male)\r\n\r\n\r\n-0.13\r\n\r\n\r\n0.004\r\n\r\n\r\n-35.56\r\n\r\n\r\n<0.001\r\n\r\n\r\nNote.  R\\(^2\\) =  0.384\r\n\r\n\r\n#Graph results\r\nplot4 <- ggplot(data = nz_1, aes(y = HLTH.Height, x = male_id)) + \r\n  geom_jitter(alpha = .2, na.rm = TRUE) + \r\n  geom_boxplot(notch = TRUE, na.rm = TRUE) +\r\n  labs(title = \"Boxplots of height by gender in NZAVS 2019 wave\") +\r\n  ylab(\"Height\") +\r\n  xlab(\"Gender\") +\r\n  theme_apa()\r\nplot4\r\n\r\n\r\n\r\n\r\nThe null hypothesis that there is no relationship between height and gender is rejected at the 5% significance level (p <0.001). There is strong evidence to suggest that there is a linear relationship between height and gender, with the fitted regression line \\[\r\n\\operatorname{\\widehat{HLTH.Height}} = 1.525 + 0.002(\\operatorname{HLTH.Weight})\r\n\\]\r\nGender is therefore a useful linear predictor of height, explaining 38.4% (R\\(^2\\) = 0.384) of the variance in the model. The boxplots show that heights are taller for males relative to non-males, suggesting that physiological development is sexually dimorphic.\r\nQuestion 5\r\n\r\n\r\n#Filter NZAVS dataset by women and rename HLTH.Height to mother_height\r\nnz_2 <- nz_1 %>%\r\n  filter(male_id==\"Not_Male\") %>%\r\n  select(HLTH.Height) %>%\r\n  dplyr::rename(mother_height = HLTH.Height)\r\n\r\n#Run linear model of Pearson and Lee (1903) dataset\r\nmod3 <- lm(daughter_height ~ mother_height, data = md_df)\r\n\r\n#Predict daughter_height of NZAVS dataset using linear model\r\npred1 <- predict(mod3, type = \"response\", interval = \"confidence\", newdata = nz_2)\r\n\r\n#Create dataframe of actual heights and predicted daughter heights\r\nnz_3 <- data.frame(nz_2, pred1)\r\n\r\n#Graph the predicted results\r\nplot5 <- ggplot(data = nz_3, \r\n                aes(x = mother_height, y = fit)) +\r\n                geom_point(na.rm = TRUE) +\r\n                geom_errorbar(aes(ymin = lwr, ymax = upr)) +\r\n                expand_limits(x = c(1,2), y = c(1,2)) +\r\n                theme_apa() +\r\n                labs(title = \"Predicted daughter's heights for NZ population of women\") +\r\n                ylab(\"Predicted daughter's height (m)\") +\r\n                xlab(\"Mother's height (m)\")\r\nplot5\r\n\r\n\r\n\r\n\r\nQuestion 6\r\n\r\n\r\n#Create year variable in NZAVS dataset\r\nnz_4 <- nz_2 %>%\r\n  dplyr::mutate(year = \"2019\") %>%\r\n  dplyr::mutate(year = as.factor(year))\r\n\r\n#Create year variable in Pearson and Lee's dataset\r\nmd_1 <- md_df %>%\r\n  select(mother_height) %>%\r\n  dplyr::mutate(year = \"1903\") %>%\r\n  dplyr::mutate(year = as.factor(year))\r\n\r\n#Combine datasets into a single dataframe\r\ncompare_heights <- rbind(md_1, nz_4)\r\n\r\n#Calculate mean and standard deviation for women's height across years and present in table\r\nstats4 <- compare_heights %>%\r\n  group_by(year) %>%\r\n  summarise(\r\n    \"m.h\" = round(mean(mother_height, na.rm = TRUE), digits = 2),\r\n    \"sd.h\" = round(sd(mother_height, na.rm = TRUE), digits = 2),\r\n    \"n\" = n()\r\n     )\r\ntab5 <- stats4 %>%\r\n  kbl(caption = \"Women's heights in Pearson and Lee's (1903) and NZAVS (2019) datasets\",\r\n      col.names = c(\"Year\",\r\n                    \"Height (M)\",\r\n                    \"Height (SD)\",\r\n                    \"n\"),\r\n      align = c(\"l\", \"r\", \"r\", \"r\")\r\n      ) %>%\r\n  kable_classic_2(c(\"striped\", \"hover\"), full_width = TRUE)\r\ntab5\r\n\r\n\r\n\r\nTable 4: Women’s heights in Pearson and Lee’s (1903) and NZAVS (2019) datasets\r\n\r\n\r\nYear\r\n\r\n\r\nHeight (M)\r\n\r\n\r\nHeight (SD)\r\n\r\n\r\nn\r\n\r\n\r\n1903\r\n\r\n\r\n1.59\r\n\r\n\r\n0.06\r\n\r\n\r\n5524\r\n\r\n\r\n2019\r\n\r\n\r\n1.65\r\n\r\n\r\n0.07\r\n\r\n\r\n1305\r\n\r\n\r\n#Graph results\r\nplot6 <- ggplot(data = compare_heights, aes(y = mother_height, x = year)) + \r\n  geom_jitter(alpha = .2, na.rm = TRUE) + \r\n  geom_boxplot(notch = TRUE, na.rm = TRUE) +\r\n  labs(title = \"Boxplots of women's heights by year\") +\r\n  ylab(\"Height\") +\r\n  xlab(\"Year\") +\r\n  theme_apa()\r\nplot6\r\n\r\n\r\n\r\n#Check equality of variances\r\ntest1 <- leveneTest(mother_height ~ year, data = compare_heights)\r\ntest1\r\n\r\n\r\nLevene's Test for Homogeneity of Variance (center = median)\r\n        Df F value    Pr(>F)    \r\ngroup    1  53.911 2.341e-13 ***\r\n      6810                      \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n#Perform two sample t-test\r\ntest2 <- t.test(mother_height ~ year, data = compare_heights, var.equal = TRUE)\r\ntest2\r\n\r\n\r\n\r\n    Two Sample t-test\r\n\r\ndata:  mother_height by year\r\nt = -32.3, df = 6810, p-value < 2.2e-16\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -0.06767758 -0.05993276\r\nsample estimates:\r\nmean in group 1903 mean in group 2019 \r\n          1.587468           1.651273 \r\n\r\n#Calculate Cohen's d\r\ntest3 <- cohen.d(mother_height ~ year, data = compare_heights, pooled = TRUE)\r\ntest3\r\n\r\n\r\n\r\nCohen's d\r\n\r\nd estimate: -0.9994302 (large)\r\n95 percent confidence interval:\r\n    lower     upper \r\n-1.062366 -0.936494 \r\n\r\nThere seems to be a difference in samples when comparing means and standard deviations for women’s heights in the 1903 and 2019 datasets. Visual inspection of boxplots support this exploratory hypothesis with women on average appearing shorter in 1903 than in 2019.\r\nTo test this hypothesis, we must first determine whether the sample variances are equal. Levene’s test was not statistically significant (p <0.001) suggesting equal variances.\r\nThe null hypothesis that there is no difference between sample heights is rejected at the 5% significance level (t = -32.3, p <0.001, d = -1). There is strong evidence to suggest that there is a difference in heights across sample years, with women in 1903 (M = 1.59, SD = 0.06) shorter on average than their modern counterparts (M = 1.65, SD = 0.07) by roughly 6 cm.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-27-workbook5/workbook5_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-06-06T16:19:12+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-20-journal4/",
    "title": "Journal 4",
    "description": "Bringing it all together",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-03-20",
    "categories": [],
    "contents": "\r\nUnfortunately I was unable to attend the lecture and workshop this week, so I am not sure if I missed important information about how to approach the workbook questions. It seemed to take me longer this week to get my code working correctly than in previous weeks; however this could perhaps just be the standard progression of difficulty across the trimester as we build upon earlier skills.\r\nI had some challenges with questions 2, 3 and 4. I knew theoretically how to find the required answer, but my goal was to try to generate the answer in one piped workflow. This took more time than it might otherwise have done, especially as I have not used piping extensively before. I kept getting errors about particular variables not being available, as I’d not selected them at the start of the pipeline. I also did not fully understand why I could get some functions to work fine outside of a piped workflow, but wouldn’t work similarly inside a pipe. I will need to do more study to find out how and why the functions are coded differently when piping them.\r\nDespite these challenges, I offered what tips I did gain in the PSYC447 Q&A Padlet. These might have appeared obvious to other students, but someone may find them useful nonetheless. Hopefully these were descriptive enough that someone else could follow the same approach I took, while still requiring the reader to actively seek understanding of each function in order to code it properly. I believe this is most beneficial for learning to code R.\r\n\r\n\r\n\r\n",
    "preview": {},
<<<<<<< HEAD
=======
=======
>>>>>>> d6cd60d30126cd8d83524678b92dfc6f4d2a1e22
    "preview": {},
    "last_modified": "2021-06-06T16:06:50+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-11-journal9/",
    "title": "Journal 9",
    "description": "Multi-level modelling",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-05-11",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-06T14:10:47+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-11-workbook9/",
    "title": "Workbook 9",
    "description": "Ordinal, cluster, multivariate and mediation models",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-05-11",
    "categories": [],
    "contents": "\r\nHere we will estimate a range of demographic and ideological predictors of attitudes to government surveillance and attitudes to regulate AI in New Zealand in 2018. The particular predictors we have selected include gender, political orientation and patriotism. We will begin by running the Bayesian multivariate model.\r\n\r\n\r\n#Run Bayesian multivariate regression model\r\n#brm(\r\n#  mvbind(Issue.GovtSurveillance, Issue.RegulateAI) ~ Gender + Pol.Orient + PATRIOT,\r\n#  data = nz2018,\r\n#  file = here::here(\"data\", \"multivariate_issues\"), \r\n#)\r\nmod1 <- readRDS(\"multivariate_issues.rds\")\r\nsummary(mod1)\r\n\r\n\r\n Family: MV(gaussian, gaussian) \r\n  Links: mu = identity; sigma = identity\r\n         mu = identity; sigma = identity \r\nFormula: Issue.GovtSurveillance ~ Gender + Pol.Orient + PATRIOT \r\n         Issue.RegulateAI ~ Gender + Pol.Orient + PATRIOT \r\n   Data: nz2018 (Number of observations: 5508) \r\nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\r\n         total post-warmup samples = 4000\r\n\r\nPopulation-Level Effects: \r\n                                 Estimate Est.Error l-95% CI u-95% CI\r\nIssueGovtSurveillance_Intercept      1.58      0.14     1.30     1.85\r\nIssueRegulateAI_Intercept            3.87      0.13     3.62     4.12\r\nIssueGovtSurveillance_Gender        -0.07      0.05    -0.16     0.03\r\nIssueGovtSurveillance_Pol.Orient     0.29      0.02     0.25     0.32\r\nIssueGovtSurveillance_PATRIOT        0.31      0.02     0.26     0.35\r\nIssueRegulateAI_Gender              -0.51      0.05    -0.60    -0.41\r\nIssueRegulateAI_Pol.Orient           0.07      0.02     0.04     0.10\r\nIssueRegulateAI_PATRIOT              0.08      0.02     0.04     0.12\r\n                                 Rhat Bulk_ESS Tail_ESS\r\nIssueGovtSurveillance_Intercept  1.00     8053     3009\r\nIssueRegulateAI_Intercept        1.00     7366     3338\r\nIssueGovtSurveillance_Gender     1.00     8562     3103\r\nIssueGovtSurveillance_Pol.Orient 1.00     7704     2665\r\nIssueGovtSurveillance_PATRIOT    1.00     8959     3293\r\nIssueRegulateAI_Gender           1.00     8651     2779\r\nIssueRegulateAI_Pol.Orient       1.00     8112     3073\r\nIssueRegulateAI_PATRIOT          1.00     7153     2754\r\n\r\nFamily Specific Parameters: \r\n                            Estimate Est.Error l-95% CI u-95% CI Rhat\r\nsigma_IssueGovtSurveillance     1.71      0.02     1.68     1.74 1.00\r\nsigma_IssueRegulateAI           1.61      0.02     1.58     1.64 1.00\r\n                            Bulk_ESS Tail_ESS\r\nsigma_IssueGovtSurveillance     7954     3204\r\nsigma_IssueRegulateAI           8575     2863\r\n\r\nResidual Correlations: \r\n                                              Estimate Est.Error\r\nrescor(IssueGovtSurveillance,IssueRegulateAI)    -0.05      0.01\r\n                                              l-95% CI u-95% CI Rhat\r\nrescor(IssueGovtSurveillance,IssueRegulateAI)    -0.08    -0.03 1.00\r\n                                              Bulk_ESS Tail_ESS\r\nrescor(IssueGovtSurveillance,IssueRegulateAI)     6350     2983\r\n\r\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\n#Plot model\r\nbrms::mcmc_plot(mod1,\r\n               type = \"areas\",\r\n               prob = .89)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-11-workbook9/workbook9_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-06-06T16:25:27+12:00",
    "input_file": "workbook9.knit.md"
  },
  {
    "path": "posts/2021-05-04-journal8/",
    "title": "Journal 8",
    "description": "Generalised linear models",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-05-04",
    "categories": [],
    "contents": "\r\nI have found the last few weeks’ workbooks to be more challenging to complete, as there is less specificity around what steps to take to produce models and interpret their fit. I have felt quite lost as to what analysis steps I need to take. I would prefer that there were some questions outlining specific variables of interest, what models to test, and what types of plots are useful to examine. Then, perhaps there could be one question where the analysis is free choice. This would reduce uncertainty about what to include in the workbook answers.\r\nThis was the first time I have produced Bayesian regression models. I was unable to compare the mdoel fit for a Poisson general linear model, and a Bayesian zero-inflated Poisson regression model. I was also unsure how to report on the output of this analysis, despite the lecture section on reporting. Although I have been keeping up in previous lectures, it felt like there was significantly more content to learn this week than before.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-05T19:40:37+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-05-04-workbook8/",
    "title": "Workbook 8",
    "description": "Generalised linear models",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-05-04",
    "categories": [],
    "contents": "\r\n\r\n\r\n#Centre and scale continuous predictors\r\nnz_1['Household.INC_s'] <- as.data.frame(scale(nz_1$Household.INC))\r\nnz_1['Age'] <- as.data.frame(scale(nz_1$Age))\r\n\r\n#Generate logistic regression model\r\nmod1 <- glm(Believe.Spirit ~ Gender + Age + Household.INC_s,\r\n            data = nz_1,\r\n            family = \"binomial\")\r\n\r\n#Plot model output\r\nstats1 <- parameters::model_parameters(mod1)\r\nprint_md(stats1)\r\n\r\n\r\nParameter\r\nLog-Odds\r\nSE\r\n95% CI\r\nz\r\np\r\n(Intercept)\r\n-1.06\r\n0.06\r\n(-1.19, -0.95)\r\n-17.44\r\n< .001\r\nGender\r\n0.75\r\n0.09\r\n(0.57, 0.93)\r\n8.01\r\n< .001\r\nAge\r\n-0.08\r\n0.05\r\n(-0.18, 0.01)\r\n-1.74\r\n0.083\r\nHousehold.INC_s\r\n0.21\r\n0.05\r\n(0.12, 0.31)\r\n4.49\r\n< .001\r\n\r\nplot(stats1)\r\n\r\n\r\n\r\n#Write model equation with coefficients\r\neq1 <- equatiomatic::extract_eq(mod1, use_coefs = TRUE)\r\nint1 <- plogis(coef(mod1)[[1]])\r\n\r\n#Plot predicted values\r\nplot(ggeffects::ggpredict(mod1, terms = c(\"Gender\", \"Age\", \"Household.INC_s\")), add.data = TRUE, alpha =.1) +\r\n  scale_y_continuous(limits = c(0, 1)) +\r\n  scale_x_continuous(limits = c(0, 1))\r\n\r\n\r\n\r\n#Check model fit\r\ncheck1 <- performance::check_model(mod1)\r\ncheck1\r\n\r\n\r\n\r\n\r\nWe fitted a logistic model (estimated using ML) to predict belief in spirit or a life-force with gender, age and household income. Here, the linear model \\[\r\n\\log\\left[ \\frac { \\widehat{P( \\operatorname{Believe.Spirit} = \\operatorname{Not\\ Believe\\ Spirit} )} }{ 1 - \\widehat{P( \\operatorname{Believe.Spirit} = \\operatorname{Not\\ Believe\\ Spirit} )} } \\right] = -1.06 + 0.75(\\operatorname{Gender}) - 0.08(\\operatorname{Age}) + 0.21(\\operatorname{Household.INC\\_s})\r\n\\] has an intercept of -1.06 which, when used to examine the cumulative density of the logistic distribution, gives an 25.6% probability that males of intermediate age and income believe in spirits or a life-force.\r\nGender is an important predictor of spirit belief (beta = 0.75, 95% CI [0.57, 0.93], p <0.001). Women are more likely to believe in spirits than men, even when accounting for age and household income. Alongside possible biological factors, there may be an effect of gender socialisation at play that lead to greater religious thinking among women.\r\nAge is also an important predictor of spirit belief (beta = -0.08, 95% CI [-0.18, 0.01], p 0.083). Older people are more likely to believe in spirits than younger people, even when accounting for gender and household income. This could potentially be a result of a generational effect e.g. decreasing religiosity among younger generations, or an individual effect e.g. people develop beliefs in spirits as they get older, perhaps in response to growing mortality.\r\nFinally, household income is an important predictor of spirit belief (beta = 0.21, 95% CI [0.12, 0.31], p <0.001). Those living in households with higher income are more likely to believe in spirits than those in households with lower income, even when accounting for gender and age. Spirituality may impact on behaviours that are linked to higher income earners such as religion-based work ethic and material reward.\r\n\r\n\r\n#Generate poisson regression model\r\nmod2 <- glm(CharityDonate ~ Gender + Age + Household.INC_s,\r\n            data = nz_1,\r\n            family = \"poisson\")\r\n\r\n#Plot model output\r\nstats2 <- parameters::model_parameters(mod2)\r\nprint_md(stats2)\r\n\r\n\r\nParameter\r\nLog-Mean\r\nSE\r\n95% CI\r\nz\r\np\r\n(Intercept)\r\n6.84\r\n8.58e-04\r\n(6.83, 6.84)\r\n7971.34\r\n< .001\r\nGender\r\n-0.02\r\n1.38e-03\r\n(-0.02, -0.01)\r\n-12.16\r\n< .001\r\nAge\r\n0.17\r\n7.09e-04\r\n(0.17, 0.17)\r\n240.12\r\n< .001\r\nHousehold.INC_s\r\n0.31\r\n2.59e-04\r\n(0.31, 0.31)\r\n1198.79\r\n< .001\r\n\r\nplot(stats2)\r\n\r\n\r\n\r\n#Write model equation with coefficients\r\neq2 <- equatiomatic::extract_eq(mod2, use_coefs = TRUE)\r\n\r\n#Plot predicted values\r\nplot(ggeffects::ggpredict(mod2, terms = c(\"Gender\", \"Age\", \"Household.INC_s\")), add.data = TRUE, alpha =.1) +\r\n  scale_y_continuous(limits = c(0, 1)) +\r\n  scale_x_continuous(limits = c(0, 1))\r\n\r\n\r\n\r\n#Check model fit\r\ncheck2 <- performance::check_model(mod2)\r\ncheck2\r\n\r\n\r\n\r\n#Check zero inflation and overdispersion\r\ncheck3 <- check_zeroinflation(mod2)\r\ncheck3\r\n\r\n\r\n# Check for zero-inflation\r\n\r\n   Observed zeros: 330\r\n  Predicted zeros: 0\r\n            Ratio: 0.00\r\n\r\ncheck4 <- check_overdispersion(mod2)\r\ncheck4\r\n\r\n\r\n# Overdispersion test\r\n\r\n       dispersion ratio =    13589.080\r\n  Pearson's Chi-Squared = 31499488.564\r\n                p-value =      < 0.001\r\n\r\nNext, we fitted a poisson model (estimated using ML) to predict charitable donations with gender, age and household income. Here, the linear model \\[\r\n\\log ({ \\widehat{E( \\operatorname{CharityDonate} )} })  = 6.84 - 0.02(\\operatorname{Gender}) + 0.17(\\operatorname{Age}) + 0.31(\\operatorname{Household.INC\\_s})\r\n\\] has an intercept of 6.84 which indicates the log of the expected donation amount in the last year ($930.27) for males of intermediate age and income.\r\nGender is an important predictor of charitable donations (beta = -0.02, 95% CI [-0.02, -0.01], p <0.001). Women donate more than men, even when accounting for age and household income. This may reflect gender socialisation factors such as an expectation to care for others.\r\nAge is also an important predictor of charitable donations (beta = 0.17, 95% CI [0.17, 0.17], p <0.001). Older people donate more than younger people, even when accounting for gender and household income. This could potentially be a result of a generational effect e.g. decreasing generosity to strangers among younger generations, or an individual effect e.g. increased discretionary income among older people.\r\nFinally, household income is an important predictor of charitable donations (beta = 0.31, 95% CI [0.31, 0.31], p <0.001). Those living in households with higher income donate more than those in households with lower income, even when accounting for gender and age. This seems to be fairly self-explpanatory, with higher income households having greater discretionary income to spend on charitable donations.\r\nChecking our model fit, we find that there is evidence of zero-inflation and overdispersion in the model. We will therefore compare the Poisson regression model with a zero-inflated Poisson regression model to see which is the better fit.\r\n\r\n\r\n#Set Charity Donation variable to integer\r\nnz_1$CharityDonate <- as.integer(nz_1$CharityDonate)\r\n\r\n#Generate zero-inflated poisson regression model\r\n#brms::brm(CharityDonate ~ Gender + Age + Household.INC_s,\r\n#                   family = \"zero_inflated_poisson\",\r\n#                   file = here::here(\"data\", \"zeroinflated_poisson_donate\"),\r\n#                   data = nz_1)\r\nbmod1 <- readRDS(\"zeroinflated_poisson_donate.rds\")\r\nstats3 <- summary(bmod1)\r\nstats3\r\n\r\n\r\n Family: zero_inflated_poisson \r\n  Links: mu = log; zi = identity \r\nFormula: CharityDonate ~ Gender + Age + Household.INC_s \r\n   Data: nz_1 (Number of observations: 5329) \r\nSamples: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\r\n         total post-warmup samples = 4000\r\n\r\nPopulation-Level Effects: \r\n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS\r\nIntercept           6.75      0.00     6.75     6.76 1.00     3145\r\nGender              0.70      0.00     0.70     0.70 1.00     2675\r\nAge                 0.31      0.00     0.31     0.31 1.00     3918\r\nHousehold.INC_s     0.23      0.00     0.23     0.23 1.00     4371\r\n                Tail_ESS\r\nIntercept           2850\r\nGender              2401\r\nAge                 2929\r\nHousehold.INC_s     3222\r\n\r\nFamily Specific Parameters: \r\n   Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\r\nzi     0.14      0.00     0.13     0.15 1.01      639      646\r\n\r\nSamples were drawn using sampling(NUTS). For each parameter, Bulk_ESS\r\nand Tail_ESS are effective sample size measures, and Rhat is the potential\r\nscale reduction factor on split chains (at convergence, Rhat = 1).\r\n\r\n#Plot predicted values from posterior samples\r\nplot(\r\n  conditional_effects(\r\n    bmod1,\r\n    spaghetti = TRUE,\r\n    nsamples = 100,\r\n    select_points = 0.1\r\n  ),\r\n  points = TRUE,\r\n  point_args = list(alpha = 0.1,\r\n                    width = .02)\r\n)\r\n\r\n\r\n\r\n#Zero-inflated poisson model posterior predictive check\r\nbrms::pp_check(bmod1) + xlim(0, 5)\r\n\r\n\r\n\r\n#Compare models\r\n#b0 <- add_criterion(mod2, \"loo\")\r\n#b1 <- add_criterion(bmod1, \"loo\")\r\n#w <- loo_compare(b0, b1, criterion = \"loo\")\r\n#w\r\n#cbind(waic_diff = w[, 1] * -2,\r\n#      se        = w[, 2] * 2)\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-05-04-workbook8/workbook8_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-06-06T16:22:07+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-27-journal7/",
    "title": "Journal 7",
    "description": "Other types of modelling",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-04-27",
    "categories": [],
    "contents": "\r\nThis week covered a whole range of statistical models, including ordinal responses and predictors, random intercept models, multivariate models and mediation.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-06T14:07:59+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-27-workbook7/",
    "title": "Workbook 7",
    "description": "Multiple regression",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-04-27",
    "categories": [],
    "contents": "\r\nI have chosen to examine five variables relating to psychological distress and micro-/macro-level economic security variables from Wave 11 (2019) of the NZ Attitudes and Values Survey (NZAVS).\r\n\r\n\r\n#Create tibble of variable means and standard deviations\r\nstats1 <- nz_1 %>%\r\n  select(KESSLER6sum,\r\n         NZ.Business.Conditions,\r\n         NZ.Economic.Situation,\r\n         Your.Future.Security,\r\n         Emp.JobSecure\r\n         ) %>%\r\n  summarise(\r\n    round(\r\n      rbind(\r\n        across(c(KESSLER6sum,\r\n                 NZ.Business.Conditions,\r\n                 NZ.Economic.Situation,\r\n                 Your.Future.Security,\r\n                 Emp.JobSecure\r\n                 ), ~mean(.x, na.rm = TRUE)\r\n               ),\r\n        across(c(KESSLER6sum,\r\n                 NZ.Business.Conditions,\r\n                 NZ.Economic.Situation,\r\n                 Your.Future.Security,\r\n                 Emp.JobSecure\r\n                 ), ~sd(.x, na.rm = TRUE)\r\n               )\r\n        ), digits = 2)\r\n    )\r\n\r\n#Create table to store variable means and standard deviations\r\ntab1 <- t(stats1) %>%\r\n  kbl(caption = \"Psychological distress and economic security variables in the NZAVS 2019 wave\",\r\n      col.names = c(\"M\",\r\n                    \"SD\"),\r\n      align = c(\"r\", \"r\")\r\n      ) %>%\r\n  kable_classic_2(c(\"striped\", \"hover\"), full_width = TRUE)\r\ntab1\r\n\r\n\r\n\r\nTable 1: Psychological distress and economic security variables in the NZAVS 2019 wave\r\n\r\n\r\n\r\n\r\nM\r\n\r\n\r\nSD\r\n\r\n\r\nKESSLER6sum\r\n\r\n\r\n4.73\r\n\r\n\r\n3.81\r\n\r\n\r\nNZ.Business.Conditions\r\n\r\n\r\n5.71\r\n\r\n\r\n1.88\r\n\r\n\r\nNZ.Economic.Situation\r\n\r\n\r\n5.56\r\n\r\n\r\n2.15\r\n\r\n\r\nYour.Future.Security\r\n\r\n\r\n6.54\r\n\r\n\r\n2.23\r\n\r\n\r\nEmp.JobSecure\r\n\r\n\r\n5.45\r\n\r\n\r\n1.56\r\n\r\n\r\n#Create density plots of variables\r\nnz_2 <- nz_1 %>%\r\n  select(KESSLER6sum,\r\n         NZ.Business.Conditions,\r\n         NZ.Economic.Situation,\r\n         Your.Future.Security,\r\n         Emp.JobSecure\r\n         ) %>%\r\n  melt()\r\nplot1 <- ggplot(data = nz_2, aes(x = value, color = variable, fill = variable)) + \r\n  geom_density(alpha = 0.7) +\r\n  facet_wrap(~ variable, nrow = 2) +\r\n  labs(title = str_wrap(\"Density plots of psychological distress and economic security variables in NZAVS 2019 wave\", 60)) +\r\n  ylab(\"Density\") +\r\n  xlab(\"Variable\") +\r\n  theme(plot.title = element_text(size=14),\r\n        axis.text.x = element_text(angle = 20, hjust = 1),\r\n        legend.position = \"none\"\r\n  )\r\nplot1\r\n\r\n\r\n\r\n#Create correlation plot of variables\r\nextrafont::loadfonts()\r\nplot2 <- nz_1 %>%\r\n  select(KESSLER6sum,\r\n         NZ.Business.Conditions,\r\n         NZ.Economic.Situation,\r\n         Your.Future.Security,\r\n         Emp.JobSecure\r\n         ) %>%\r\n  correlation(partial = FALSE, multilevel = FALSE) %>%\r\n  plot()\r\nplot2\r\n\r\n\r\n\r\n\r\nPsychological distress was measured using the Kessler 6. The six items contribute to a total possible score of 24, however the average amongst the general NZ population is relatively low (M = 4.73, SD = 3.81). The density plot shows a platykurtic distribution with positive skew, indicating the bulk of scores were low but with a few high-scoring individuals.\r\nPersonal and national economic wellbeing factors were measured on a scale from 0 (“Completely Dissatisfied”) to 10 (“Completely Satisfied”). Satisfaction with NZ business conditions (M = 5.71, SD = 1.88), the NZ economy (M = 5.56, SD = 2.15) and perceived future security (M = 6.54, SD = 2.23) were relatively mid-range. Density plots show reasonably similar distributions.\r\nPerceived job security was measured on a scale from 1 (“Not secure”) to 7 (“Very secure”) with slightly higher average responses (M = 5.45, SD = 1.56). The density plot shows a leptokurtic distribution with negative skew.\r\nA correlation plot shows that all four economic security variables are positively correlated with each other, with perceived job security having the weakest correlation. Psychological distress was negatively correlated with the four economic security variables indicating that as perceived economic security increases, distress decreases.\r\n\r\n\r\n#Run linear model with one predictor\r\nmod1 <- lm(KESSLER6sum ~ NZ.Economic.Situation.c, data = nz_1)\r\ntab2 <- parameters::model_parameters(mod1)\r\nprint_md(tab2)\r\n\r\n\r\nParameter\r\nCoefficient\r\nSE\r\n95% CI\r\nt(2058)\r\np\r\n(Intercept)\r\n4.71\r\n0.08\r\n(4.55, 4.87)\r\n57.37\r\n< .001\r\nNZ.Economic.Situation.c\r\n-0.34\r\n0.04\r\n(-0.41, -0.26)\r\n-8.81\r\n< .001\r\n\r\n#Plot linear model\r\nplot3 <- plot(ggeffects::ggpredict(mod1,\r\n                                   terms = \"NZ.Economic.Situation.c\"\r\n                                   ),\r\n              add.data = TRUE,\r\n              jitter = 0.2,\r\n              dot.alpha =.2,\r\n              ) +\r\n  scale_x_continuous(limits = c(-5, 5)) +\r\n  scale_y_continuous(limits = c(0, 7)) +\r\n  theme_classic()\r\nplot3\r\n\r\n\r\n\r\n#Write model equation with coefficients\r\neq1 <- equatiomatic::extract_eq(mod1,  use_coefs = TRUE)\r\neq1\r\n\r\n\r\n\\[\r\n\\operatorname{\\widehat{KESSLER6sum}} = 4.71 - 0.34(\\operatorname{NZ.Economic.Situation.c})\r\n\\]\r\n\r\nHere, the linear model \\[\r\n\\operatorname{\\widehat{KESSLER6sum}} = 4.71 - 0.34(\\operatorname{NZ.Economic.Situation.c})\r\n\\] has an intercept of 4.71 which indicates the expected score when satisfaction with the NZ economy is the average score.\r\nSatisfaction with the NZ economy is an important predictor of psychological distress (beta = -0.34, 95% CI = 0.95, p <0.001).\r\n\r\n\r\n#Run linear model with two predictors\r\nmod2 <- lm(KESSLER6sum ~ NZ.Economic.Situation.c + Your.Future.Security.c, data = nz_1)\r\ntab3 <- parameters::model_parameters(mod2)\r\nprint_md(tab3)\r\n\r\n\r\nParameter\r\nCoefficient\r\nSE\r\n95% CI\r\nt(2054)\r\np\r\n(Intercept)\r\n4.70\r\n0.08\r\n(4.55, 4.85)\r\n60.68\r\n< .001\r\nNZ.Economic.Situation.c\r\n-0.09\r\n0.04\r\n(-0.17, -0.01)\r\n-2.29\r\n0.022\r\nYour.Future.Security.c\r\n-0.60\r\n0.04\r\n(-0.68, -0.53)\r\n-16.04\r\n< .001\r\n\r\n#Plot linear model\r\nplot4 <- plot(ggeffects::ggpredict(mod2,\r\n                                   terms = c(\"NZ.Economic.Situation.c\",\r\n                                             \"Your.Future.Security.c\")\r\n                                   ),\r\n              add.data = TRUE,\r\n              jitter = 0.2,\r\n              dot.alpha =.2,\r\n              ) +\r\n  scale_x_continuous(limits = c(-5, 5)) +\r\n  scale_y_continuous(limits = c(0, 7)) +\r\n  theme_classic()\r\nplot4\r\n\r\n\r\n\r\n#Write model equation with coefficients\r\neq2 <- equatiomatic::extract_eq(mod2,  use_coefs = TRUE)\r\neq2\r\n\r\n\r\n\\[\r\n\\operatorname{\\widehat{KESSLER6sum}} = 4.7 - 0.09(\\operatorname{NZ.Economic.Situation.c}) - 0.6(\\operatorname{Your.Future.Security.c})\r\n\\]\r\n\r\nHere, the linear model \\[\r\n\\operatorname{\\widehat{KESSLER6sum}} = 4.7 - 0.09(\\operatorname{NZ.Economic.Situation.c}) - 0.6(\\operatorname{Your.Future.Security.c})\r\n\\] has an intercept of 4.7 which indicates the expected score when satisfaction with the NZ economy is the average score.\r\nSatisfaction with the NZ economy is no longer an important predictor of psychological distress (beta = -0.09, 95% CI = 0.95, p <0.001) as its explanatory power is obscured by perceived future security (beta = -0.6, 95% CI = 0.95, p <0.001).\r\n\r\n\r\n#Compare models\r\nmodcomp <- performance::compare_performance(mod1,mod2)\r\nmodcomp\r\n\r\n\r\n# Comparison of Model Performance Indices\r\n\r\nName | Model |       AIC |       BIC |    R2 | R2 (adj.) |  RMSE | Sigma\r\n------------------------------------------------------------------------\r\nmod1 |    lm | 11272.048 | 11288.939 | 0.036 |     0.036 | 3.727 | 3.729\r\nmod2 |    lm | 11013.321 | 11035.837 | 0.143 |     0.143 | 3.512 | 3.514\r\n\r\nplot(modcomp)\r\n\r\n\r\n\r\n\r\nComparing the two models, the second model with two predictors appears to be the better fit for the data. AIC, BIC and RMSE both decreased for the second model, while the R^2 value increased. Inclusion of perceived future security is therefore a better predictor of psychological distress than satisfaction with the NZ economy alone.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-04-27-workbook7/workbook7_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-06-06T16:21:04+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-20-journal6/",
    "title": "Journal 6",
    "description": "Reporting success",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-04-20",
    "categories": [],
    "contents": "\r\nI opted to write my report introduction using Papaja. I was largely successful at using the citation skills from the first assignment, however I was unable to seperate out the R package references from appearing in the reference list. I am unsure whether it is accepted APA practice to include technical references i.e. programmes in a separate list. For this reason it looks slightly messy!\r\nI would like to better understand how to integrate tables and graphs within Papaja. Some of the preliminary data findings could have been better represented in this way. However, I had difficulty using the Kbl() function as I had in my previous workbook. There must be Papaja-specific functions that enable these to work.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-05T19:38:10+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-04-20-workbook6/",
    "title": "Workbook 6",
    "description": "Prototypicality, discrimination and Maori identity",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-04-20",
    "categories": [],
    "contents": "\r\nDoes prototypicality of group membership moderate the effect of intergroup and intragroup discrimination on ethnic identity in Māori?\r\nMāori culture has been posited (e.g. by Durie, 1998, 2001) as a protective factor for psychological wellbeing. Strengthening cultural/ethnic identity may be a means to addessing disparities in health, education and other contexts. However, not all people who identify as having whakapapa Māori (Māori ancestry) have the access, means or desire to adopt a traditional, “fixed” Māori identity (McIntosh, 2005). In this project I will examine the heterogeneity of Māori cultural/ethnic identity formation through participants’ experiences of discrimination by Pākehā/Tauiwi and Māori.\r\nBranscombe, Schmitt and Harvey’s (1999) rejection-identification model proposes that perceived discrimination by the dominant group leads to strengthened ingroup identification, as a defense mechanism to protect the individual’s psychological wellbeing. Evidence for this model is mixed with some studies showing increased ethnic identity while others do not (Chávez & French, 2007; Evans et al., 2014; Masuoka, 2006; Pérez et al., 2008; Romero & Roberts, 2003). One explanation for these mixed results may be that some individuals are more or less able to traverse boundaries of group category membership because of how prototypical they appear or behave. Evidence suggests that self-rated similarity to the average ingroup member mediates the relationship between perceived discrimination and life satisfaction [giamo-2012]. However, research to date has not examined the effect of group membership prototypicality on the relationship between perceived discrimination and ethnic identity. If highly prototypical group members are less able than low prototypicality members to distance themselves from the group category for which they are being targeted for discrimination, then they may have more reason to defend whatever limited identity choices are available to them.\r\nI want to explore whether the diversity of participants’ self-rated prototypicality to the group category ‘Māori’ based on physical, cultural and social characteristics affects how discrimination experiences are rejected or internalised through strengthened or decreased identification as Māori. The main research questions are:\r\nDoes prototypicality of group membership moderate the effect of perceived discrimination by Pākehā/Tauiwi, on Māori identity?\r\nDoes prototypicality of group membership moderate the effect of perceived discrimination by Māori, on Māori identity?\r\nMethod\r\nParticipants\r\nParticipants were 669 members of the community who identified as Māori (62.9% female, 37.1% male; Meanage=44.4, SDage=13) recruited through the New Zealand Attitudes and Values Survey.\r\nMaterial\r\nParticipants completed a range of demographic questions and construct scales. The constructs used in the current study are included below.\r\nPerceived discrimination. Participants were asked to evaluate the single item, “I feel that I am often discriminated against because of my ethnicity” on a scale from 1 (Strongly Disagree) to 7 (Strongly Agree).\r\nPerceived appearance. This subscale of Houkamau and Sibley’s (2015) MMM-ICE2 examines self-rated prototypicality of physical appearance to the Māori ethnic group. Seven items including “I think it is easy to tell that I am Māori just by looking at me” and “People would never know that I am of Māori descent just by looking at me” are rated on a scale from 1 (Strongly Disagree) to 7 (Strongly Agree).\r\nGroup membership and evaluation. Another subscale of the MMM-ICE2 examines affective and centrality components of ethnic group membership (i.e. identity). Eight items including “I love the fact I am Māori” and “My Māori ancestry is important to me” are rated on a scale from 1 (Strongly Disagree) to 7 (Strongly Agree).\r\nProcedure\r\nParticipants were posted a copy of the questionnaire, with a second postal follow-up two months later. Participants who provided an email address were also emailed and invited to complete an online version if they preferred.\r\nTo boost sample size and increase sample diversity for subsequent waves, booster samples using different sample frames were conducted. Booster sampling was conducted without replacement (i.e., all people included in previous sample frames were identified and removed from the electoral roll before generation of the new sample frames).\r\nThe fifth sample frame consisted of 9,000 people randomly selected from those who indicated on the 2012 Electoral Roll that they were of Māori ethnicit y (ethnic affiliation as Māori is listed on the roll, but other ethnic affiliations are not). A total of 689 participants responded to this booster sample (adjusted response rate = 7.78%). The questionnaire administered to the Māori booster sample included questions specifically designed for Māori.\r\n\r\n\r\n\r\nBranscombe, N. R., Schmitt, M. T., & Harvey, R. D. (1999). Perceiving pervasive discrimination among african americans: Implications for group identification and well-being. Journal of Personality and Social Psychology, 77(1), 135–149. https://doi.org/10.1037/0022-3514.77.1.135\r\n\r\n\r\nChávez, N. R., & French, S. E. (2007). Ethnicity‐related stressors and mental health in latino americans: The moderating role of parental racial socialization. Journal of Applied Social Psychology, 37(9), 1974–1998. https://doi.org/10.1111/j.1559-1816.2007.00246.x\r\n\r\n\r\nDurie, M. (1998). Whaiora: Māori health development (2nd ed.). Oxford University Press.\r\n\r\n\r\nDurie, M. (2001). Mauri ora: The dynamics of māori health. Oxford University Press.\r\n\r\n\r\nEvans, C. B. R., Smokowski, P. R., & Cotter, K. L. (2014). Individual characteristics, microsystem factors, and proximal relationship processes associated with ethnic identity in rural youth. Journal of the Society for Social Work and Research, 5(1), 45–77. https://doi.org/10.1086/675848\r\n\r\n\r\nHoukamau, C. A., & Sibley, C. G. (2015). The revised multidimensional model of māori identity and cultural engagement (MMM-ICE2). Social Indicators Research, 122(1), 279–296. https://doi.org/10.1007/s11205-014-0686-7\r\n\r\n\r\nMasuoka, N. (2006). Together they become one: Examining the predictors of panethnic group consciousness among asian americans and latinos. Social Science Quarterly, 87(5), 993–1011. https://doi.org/10.1111/j.1540-6237.2006.00412.x\r\n\r\n\r\nMcIntosh, T. (2005). Māori identities: Fixed, fluid, forced. In J. H. Liu, T. McCreanor, T. McIntosh, & T. Teaiwa (Eds.), New zealand identities: Departures and destinations (pp. 67–94). Victoria University Press.\r\n\r\n\r\nPérez, D. J., Fortuna, L., & Alegria, M. (2008). Prevalence and correlates of everyday discrimination among u.s. latinos. Journal of Community Psychology, 36(4), 421–433. https://doi.org/10.1002/jcop.20221\r\n\r\n\r\nRomero, A. J., & Roberts, R. E. (2003). The impact of multiple dimensions of ethnic identity on discrimination and adolescents’ self-esteem. Journal of Applied Social Psychology, 33(11), 2288–2305. https://doi.org/10.1111/j.1559-1816.2003.tb01885.x\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-06T15:18:28+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-27-journal5/",
    "title": "Journal 5",
    "description": "Linear models",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-03-27",
    "categories": [],
    "contents": "\r\nI’m quite pleased with how things turned out in this week’s workbook. Although the assessment was intended to be a lighter workload than last week’s, this actually provided more opportunity to try out different code to see what would be more efficient or give tidier output. For example, the instructions to simply print a table and a graph allowed me to test out the kbl() function and its arguments. I did however struggle with formatting the table output to APA standards e.g. italicising column labels.\r\nI also tried using inline call functions to pull statistics directly into my written analysis. I believe this will save a lot of time going forward, as it means I can create template reports before conducting analysis of the dataset.\r\nMy primary concern at this stage is preparation for the final report assessment. I would like to simulate data for my 489 project which is still in the data collection phase. My research question is whether prototypicality of group membership moderates the effect of perceived discrimination on ethnic identity in Māori (i.e. the rejection-identification model). As such, I am not sure how to best simulate data where there are two expected groups of high and low prototypicality participants who may or may not differ in the strength of linear relationship between discrimination and identity. If this is not possible, I would also be open to conducting the same type of moderation analysis on another dataset so that at least I will have the correct code to use for my 489.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-06-05T19:29:03+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-27-workbook5/",
    "title": "Workbook 5",
    "description": "Linear models",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-03-27",
    "categories": [],
    "contents": "\r\n\r\n\r\n#Read data\r\nnz <- readr::read_delim(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/nz/nz.csv\"),\r\n                        delim = \";\",\r\n                        locale=locale(decimal_mark = \",\"),\r\n                        col_types = cols())\r\n\r\n#Re-level Kessler 6 variables and filter dataset by 2019 survey wave\r\nf<-c(\"None Of The Time\",\r\n     \"A Little Of The Time\",\r\n     \"Some Of The Time\",\r\n     \"Most Of The Time\",\r\n     \"All Of The Time\")\r\nnz_1 <- nz %>%\r\n  dplyr::mutate_if(is.character, factor) %>%\r\n  select(\r\n    -c(\r\n      SWB.Kessler01,\r\n      SWB.Kessler02,\r\n      SWB.Kessler03,\r\n      SWB.Kessler04,\r\n      SWB.Kessler05,\r\n      SWB.Kessler06\r\n    )\r\n  ) %>%\r\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\r\n  mutate(FeelHopeless = forcats::fct_relevel(FeelHopeless, f)) %>%\r\n  mutate(FeelDepressed = forcats::fct_relevel(FeelDepressed, f)) %>%\r\n  mutate(FeelRestless = forcats::fct_relevel(FeelRestless, f)) %>%\r\n  mutate(EverythingIsEffort = forcats::fct_relevel(EverythingIsEffort, f)) %>%\r\n  mutate(FeelWorthless = forcats::fct_relevel(FeelWorthless, f)) %>%\r\n  mutate(FeelNervous = forcats::fct_relevel(FeelNervous, f)) %>%\r\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\r\n  dplyr::mutate(male_id = as.factor(Male)) %>%\r\n  dplyr::mutate(date = make_date(year = 2009, month = 6, day = 30) + TSCORE) %>%\r\n  filter(Wave==\"2019\")\r\n\r\n#Read data\r\nmd_df <- data.frame(read.table(url(\"https://raw.githubusercontent.com/avehtari/ROS-Examples/master/PearsonLee/data/MotherDaughterHeights.txt\"),\r\n                               header=TRUE))\r\n\r\n#Centre mother_height; convert daughter_height and mother_height to metres\r\nmd_df <- md_df %>%\r\n  dplyr::mutate(mother_height_c = as.numeric(scale(mother_height, center = TRUE, scale = FALSE))) %>%\r\n  dplyr::mutate(daughter_height = conv_unit(daughter_height, \"inch\", \"m\")) %>%\r\n  dplyr::mutate(mother_height = conv_unit(mother_height, \"inch\", \"m\"))\r\n\r\n\r\n\r\nQuestion 1\r\n\r\n\r\n#Present means and standard deviations for weight and height in a table\r\nstats1 <- nz_1 %>%\r\n  select(HLTH.Weight, HLTH.Height, male_id) %>%\r\n  filter(!is.na(male_id)) %>%\r\n  summarise(\r\n    \"male_id\" = as.factor(\"Total\"),\r\n    \"m.w\" = round(mean(HLTH.Weight, na.rm = TRUE), digits = 1),\r\n    \"sd.w\" = round(sd(HLTH.Weight, na.rm = TRUE), digits = 1),\r\n    \"m.h\" = round(mean(HLTH.Height, na.rm = TRUE), digits = 2),\r\n    \"sd.h\" = round(sd(HLTH.Height, na.rm = TRUE), digits = 2),\r\n    \"n\" = n()\r\n    )\r\nstats2 <- nz_1 %>%\r\n  select(HLTH.Weight, HLTH.Height, male_id) %>%\r\n  filter(!is.na(male_id)) %>%\r\n  group_by(male_id) %>%\r\n  summarise(\r\n    \"m.w\" = round(mean(HLTH.Weight, na.rm = TRUE), digits = 1),\r\n    \"sd.w\" = round(sd(HLTH.Weight, na.rm = TRUE), digits = 1),\r\n    \"m.h\" = round(mean(HLTH.Height, na.rm = TRUE), digits = 2),\r\n    \"sd.h\" = round(sd(HLTH.Height, na.rm = TRUE), digits = 2),\r\n    \"n\" = n()\r\n    )\r\nstats3 <- rbind(stats1, stats2)\r\ntab1 <- stats3 %>%\r\n  kbl(caption = \"Participants' weights and heights by gender in NZAVS 2019 wave\",\r\n      col.names = c(\"Gender\",\r\n                    \"Weight (M)\",\r\n                    \"Weight (SD)\",\r\n                    \"Height (M)\",\r\n                    \"Height (SD)\",\r\n                    \"n\"),\r\n      align = c(\"r\", \"r\", \"r\", \"r\")\r\n      ) %>%\r\n  kable_classic_2(c(\"striped\", \"hover\"), full_width = TRUE)\r\ntab1\r\n\r\n\r\n\r\nTable 1: Participants’ weights and heights by gender in NZAVS 2019 wave\r\n\r\n\r\nGender\r\n\r\n\r\nWeight (M)\r\n\r\n\r\nWeight (SD)\r\n\r\n\r\nHeight (M)\r\n\r\n\r\nHeight (SD)\r\n\r\n\r\nn\r\n\r\n\r\nTotal\r\n\r\n\r\n79.7\r\n\r\n\r\n18.8\r\n\r\n\r\n1.70\r\n\r\n\r\n0.10\r\n\r\n\r\n2057\r\n\r\n\r\nMale\r\n\r\n\r\n88.2\r\n\r\n\r\n17.1\r\n\r\n\r\n1.78\r\n\r\n\r\n0.08\r\n\r\n\r\n752\r\n\r\n\r\nNot_Male\r\n\r\n\r\n74.7\r\n\r\n\r\n18.0\r\n\r\n\r\n1.65\r\n\r\n\r\n0.07\r\n\r\n\r\n1305\r\n\r\n\r\n#Generate notched boxplots of weight and height\r\nplot1 <- ggplot(nz_1, aes(y = HLTH.Weight)) +\r\n  geom_boxplot(notch = TRUE, na.rm = TRUE) +\r\n  theme_apa() +\r\n  theme(axis.text.x = element_blank(),\r\n        axis.ticks.x = element_blank()\r\n        ) +\r\n  ylab(\"Weight (kg)\")\r\n\r\nplot2 <- ggplot(nz_1, aes(y = HLTH.Height)) +\r\n  geom_boxplot(notch = TRUE, na.rm = TRUE) +\r\n  theme_apa() +\r\n  theme(axis.text.x = element_blank(),\r\n        axis.ticks.x = element_blank()\r\n        ) +\r\n  ylab(\"Height (m)\")\r\n\r\n#Combine boxplots into one figure\r\nplot1 + plot2 + \r\n  plot_annotation(title = \"Participants' weights and heights in NZAVS 2019 wave\",\r\n                  tag_levels = 'a')\r\n\r\n\r\n\r\n\r\nQuestion 2\r\nThe mean weight of participants was 79.7 kilograms (SD = 18.8). Males were heavier than average (M = 88.2, SD = 17.1) while non-males were lighter than average (M = 74.7, SD = 18).\r\nThe mean height of participants was 1.7 metres (SD = 0.1). Males were taller than average (M = 1.78, SD = 0.08) while non-males were lighter than average (M = 1.65, SD = 0.07).\r\nQuestion 3\r\n\r\n\r\n#Run linear model of height by weight in the NZAVS dataset\r\nmod1 <- lm(HLTH.Height ~ HLTH.Weight, data = nz_1)\r\np1 <- scales::pvalue((tidy(mod1)$p.value[2]))\r\nrs1 <- round(summary(mod1)$r.squared, 3)\r\n\r\n#Present results in a table\r\ntab3 <- mod1 %>%\r\n  tidy() %>%\r\n  mutate(p.value = scales::pvalue(p.value),\r\n        term = c(\"Intercept\", \"Weight\")\r\n  ) %>%\r\n  kbl(caption = \"Linear regression of height by weight in NZAVS 2019 wave\",\r\n      col.names = c(\"Predictor\", \"B\", \"SE\", \"t\", \"p\"),\r\n      digits = c(0, 3, 3, 2, 3),\r\n      align = c(\"l\", \"r\", \"r\", \"r\", \"r\")\r\n  ) %>%\r\n  footnote(\r\n    general = c(\"R$^2$ = \", round(summary(mod1)$r.squared, 3)),\r\n    general_title = \"Note.\",\r\n    footnote_as_chunk = TRUE,\r\n    escape = FALSE\r\n  ) %>%\r\n  kable_classic_2(c(\"striped\", \"hover\"), full_width = TRUE)\r\ntab3\r\n\r\n\r\n\r\nTable 2: Linear regression of height by weight in NZAVS 2019 wave\r\n\r\n\r\nPredictor\r\n\r\n\r\nB\r\n\r\n\r\nSE\r\n\r\n\r\nt\r\n\r\n\r\np\r\n\r\n\r\nIntercept\r\n\r\n\r\n1.525\r\n\r\n\r\n0.009\r\n\r\n\r\n175.01\r\n\r\n\r\n<0.001\r\n\r\n\r\nWeight\r\n\r\n\r\n0.002\r\n\r\n\r\n0.000\r\n\r\n\r\n20.32\r\n\r\n\r\n<0.001\r\n\r\n\r\nNote.  R\\(^2\\) =  0.169\r\n\r\n\r\n#Graph results\r\nplot3 <- ggplot(data = nz_1, aes(y = HLTH.Height, x = HLTH.Weight)) + \r\n  geom_jitter(alpha = .2, na.rm = TRUE) + \r\n  geom_smooth(method = \"lm\", se = FALSE, col = \"black\", na.rm = TRUE) +\r\n  labs(title = \"Linear regression of height by weight in NZAVS 2019 wave\") +\r\n  ylab(\"Height\") +\r\n  xlab(\"Weight\") +\r\n  theme_apa()\r\nplot3\r\n\r\n\r\n\r\n\r\nThe null hypothesis that there is no relationship between height and weight is rejected at the 5% significance level (p <0.001). There is strong evidence to suggest that there is a linear relationship between height and weight, with the fitted regression line \\[\r\n\\operatorname{\\widehat{HLTH.Height}} = 1.525 + 0.002(\\operatorname{HLTH.Weight})\r\n\\]\r\nWeight is therefore a useful linear predictor of height, explaining 16.9% (R\\(^2\\) = 0.169) of the variance in the model. The scatterplot with fitted regression line shows that height increases as weight increases, suggesting that physiological development of these factors occurs in tandem.\r\nQuestion 4\r\n\r\n\r\n#Run linear model of height by gender in the NZAVS dataset\r\nmod2 <- lm(HLTH.Height ~ male_id, data = nz_1)\r\np2 <- scales::pvalue((tidy(mod2)$p.value[2]))\r\nrs2 <- round(summary(mod2)$r.squared, 3)\r\n\r\n#Present results in a table\r\ntab4 <- mod2 %>%\r\n  tidy() %>%\r\n  mutate(p.value = scales::pvalue(p.value),\r\n        term = c(\"Intercept\", \"Gender (not male)\")\r\n  ) %>%\r\n  kbl(caption = \"Linear regression of height by gender in NZAVS 2019 wave\",\r\n      col.names = c(\"Predictor\", \"B\", \"SE\", \"t\", \"p\"),\r\n      digits = c(0, 2, 3, 2, 3),\r\n      align = c(\"l\", \"r\", \"r\", \"r\", \"r\")\r\n  ) %>%\r\n  footnote(\r\n    general = c(\"R$^2$ = \", round(summary(mod2)$r.squared, 3)),\r\n    general_title = \"Note.\",\r\n    footnote_as_chunk = TRUE,\r\n    escape = FALSE\r\n  ) %>%\r\n  kable_classic_2(c(\"striped\", \"hover\"), full_width = TRUE)\r\ntab4\r\n\r\n\r\n\r\nTable 3: Linear regression of height by gender in NZAVS 2019 wave\r\n\r\n\r\nPredictor\r\n\r\n\r\nB\r\n\r\n\r\nSE\r\n\r\n\r\nt\r\n\r\n\r\np\r\n\r\n\r\nIntercept\r\n\r\n\r\n1.78\r\n\r\n\r\n0.003\r\n\r\n\r\n628.10\r\n\r\n\r\n<0.001\r\n\r\n\r\nGender (not male)\r\n\r\n\r\n-0.13\r\n\r\n\r\n0.004\r\n\r\n\r\n-35.56\r\n\r\n\r\n<0.001\r\n\r\n\r\nNote.  R\\(^2\\) =  0.384\r\n\r\n\r\n#Graph results\r\nplot4 <- ggplot(data = nz_1, aes(y = HLTH.Height, x = male_id)) + \r\n  geom_jitter(alpha = .2, na.rm = TRUE) + \r\n  geom_boxplot(notch = TRUE, na.rm = TRUE) +\r\n  labs(title = \"Boxplots of height by gender in NZAVS 2019 wave\") +\r\n  ylab(\"Height\") +\r\n  xlab(\"Gender\") +\r\n  theme_apa()\r\nplot4\r\n\r\n\r\n\r\n\r\nThe null hypothesis that there is no relationship between height and gender is rejected at the 5% significance level (p <0.001). There is strong evidence to suggest that there is a linear relationship between height and gender, with the fitted regression line \\[\r\n\\operatorname{\\widehat{HLTH.Height}} = 1.525 + 0.002(\\operatorname{HLTH.Weight})\r\n\\]\r\nGender is therefore a useful linear predictor of height, explaining 38.4% (R\\(^2\\) = 0.384) of the variance in the model. The boxplots show that heights are taller for males relative to non-males, suggesting that physiological development is sexually dimorphic.\r\nQuestion 5\r\n\r\n\r\n#Filter NZAVS dataset by women and rename HLTH.Height to mother_height\r\nnz_2 <- nz_1 %>%\r\n  filter(male_id==\"Not_Male\") %>%\r\n  select(HLTH.Height) %>%\r\n  dplyr::rename(mother_height = HLTH.Height)\r\n\r\n#Run linear model of Pearson and Lee (1903) dataset\r\nmod3 <- lm(daughter_height ~ mother_height, data = md_df)\r\n\r\n#Predict daughter_height of NZAVS dataset using linear model\r\npred1 <- predict(mod3, type = \"response\", interval = \"confidence\", newdata = nz_2)\r\n\r\n#Create dataframe of actual heights and predicted daughter heights\r\nnz_3 <- data.frame(nz_2, pred1)\r\n\r\n#Graph the predicted results\r\nplot5 <- ggplot(data = nz_3, \r\n                aes(x = mother_height, y = fit)) +\r\n                geom_point(na.rm = TRUE) +\r\n                geom_errorbar(aes(ymin = lwr, ymax = upr)) +\r\n                expand_limits(x = c(1,2), y = c(1,2)) +\r\n                theme_apa() +\r\n                labs(title = \"Predicted daughter's heights for NZ population of women\") +\r\n                ylab(\"Predicted daughter's height (m)\") +\r\n                xlab(\"Mother's height (m)\")\r\nplot5\r\n\r\n\r\n\r\n\r\nQuestion 6\r\n\r\n\r\n#Create year variable in NZAVS dataset\r\nnz_4 <- nz_2 %>%\r\n  dplyr::mutate(year = \"2019\") %>%\r\n  dplyr::mutate(year = as.factor(year))\r\n\r\n#Create year variable in Pearson and Lee's dataset\r\nmd_1 <- md_df %>%\r\n  select(mother_height) %>%\r\n  dplyr::mutate(year = \"1903\") %>%\r\n  dplyr::mutate(year = as.factor(year))\r\n\r\n#Combine datasets into a single dataframe\r\ncompare_heights <- rbind(md_1, nz_4)\r\n\r\n#Calculate mean and standard deviation for women's height across years and present in table\r\nstats4 <- compare_heights %>%\r\n  group_by(year) %>%\r\n  summarise(\r\n    \"m.h\" = round(mean(mother_height, na.rm = TRUE), digits = 2),\r\n    \"sd.h\" = round(sd(mother_height, na.rm = TRUE), digits = 2),\r\n    \"n\" = n()\r\n     )\r\ntab5 <- stats4 %>%\r\n  kbl(caption = \"Women's heights in Pearson and Lee's (1903) and NZAVS (2019) datasets\",\r\n      col.names = c(\"Year\",\r\n                    \"Height (M)\",\r\n                    \"Height (SD)\",\r\n                    \"n\"),\r\n      align = c(\"l\", \"r\", \"r\", \"r\")\r\n      ) %>%\r\n  kable_classic_2(c(\"striped\", \"hover\"), full_width = TRUE)\r\ntab5\r\n\r\n\r\n\r\nTable 4: Women’s heights in Pearson and Lee’s (1903) and NZAVS (2019) datasets\r\n\r\n\r\nYear\r\n\r\n\r\nHeight (M)\r\n\r\n\r\nHeight (SD)\r\n\r\n\r\nn\r\n\r\n\r\n1903\r\n\r\n\r\n1.59\r\n\r\n\r\n0.06\r\n\r\n\r\n5524\r\n\r\n\r\n2019\r\n\r\n\r\n1.65\r\n\r\n\r\n0.07\r\n\r\n\r\n1305\r\n\r\n\r\n#Graph results\r\nplot6 <- ggplot(data = compare_heights, aes(y = mother_height, x = year)) + \r\n  geom_jitter(alpha = .2, na.rm = TRUE) + \r\n  geom_boxplot(notch = TRUE, na.rm = TRUE) +\r\n  labs(title = \"Boxplots of women's heights by year\") +\r\n  ylab(\"Height\") +\r\n  xlab(\"Year\") +\r\n  theme_apa()\r\nplot6\r\n\r\n\r\n\r\n#Check equality of variances\r\ntest1 <- leveneTest(mother_height ~ year, data = compare_heights)\r\ntest1\r\n\r\n\r\nLevene's Test for Homogeneity of Variance (center = median)\r\n        Df F value    Pr(>F)    \r\ngroup    1  53.911 2.341e-13 ***\r\n      6810                      \r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\n#Perform two sample t-test\r\ntest2 <- t.test(mother_height ~ year, data = compare_heights, var.equal = TRUE)\r\ntest2\r\n\r\n\r\n\r\n    Two Sample t-test\r\n\r\ndata:  mother_height by year\r\nt = -32.3, df = 6810, p-value < 2.2e-16\r\nalternative hypothesis: true difference in means is not equal to 0\r\n95 percent confidence interval:\r\n -0.06767758 -0.05993276\r\nsample estimates:\r\nmean in group 1903 mean in group 2019 \r\n          1.587468           1.651273 \r\n\r\n#Calculate Cohen's d\r\ntest3 <- cohen.d(mother_height ~ year, data = compare_heights, pooled = TRUE)\r\ntest3\r\n\r\n\r\n\r\nCohen's d\r\n\r\nd estimate: -0.9994302 (large)\r\n95 percent confidence interval:\r\n    lower     upper \r\n-1.062366 -0.936494 \r\n\r\nThere seems to be a difference in samples when comparing means and standard deviations for women’s heights in the 1903 and 2019 datasets. Visual inspection of boxplots support this exploratory hypothesis with women on average appearing shorter in 1903 than in 2019.\r\nTo test this hypothesis, we must first determine whether the sample variances are equal. Levene’s test was not statistically significant (p <0.001) suggesting equal variances.\r\nThe null hypothesis that there is no difference between sample heights is rejected at the 5% significance level (t = -32.3, p <0.001, d = -1). There is strong evidence to suggest that there is a difference in heights across sample years, with women in 1903 (M = 1.59, SD = 0.06) shorter on average than their modern counterparts (M = 1.65, SD = 0.07) by roughly 6 cm.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-27-workbook5/workbook5_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-06-06T16:19:12+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-20-journal4/",
    "title": "Journal 4",
    "description": "Bringing it all together",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-03-20",
    "categories": [],
    "contents": "\r\nUnfortunately I was unable to attend the lecture and workshop this week, so I am not sure if I missed important information about how to approach the workbook questions. It seemed to take me longer this week to get my code working correctly than in previous weeks; however this could perhaps just be the standard progression of difficulty across the trimester as we build upon earlier skills.\r\nI had some challenges with questions 2, 3 and 4. I knew theoretically how to find the required answer, but my goal was to try to generate the answer in one piped workflow. This took more time than it might otherwise have done, especially as I have not used piping extensively before. I kept getting errors about particular variables not being available, as I’d not selected them at the start of the pipeline. I also did not fully understand why I could get some functions to work fine outside of a piped workflow, but wouldn’t work similarly inside a pipe. I will need to do more study to find out how and why the functions are coded differently when piping them.\r\nDespite these challenges, I offered what tips I did gain in the PSYC447 Q&A Padlet. These might have appeared obvious to other students, but someone may find them useful nonetheless. Hopefully these were descriptive enough that someone else could follow the same approach I took, while still requiring the reader to actively seek understanding of each function in order to code it properly. I believe this is most beneficial for learning to code R.\r\n\r\n\r\n\r\n",
    "preview": {},
<<<<<<< HEAD
=======
>>>>>>> eb5af2de0acf656b5cae988d04884f632db2a5b4
>>>>>>> d6cd60d30126cd8d83524678b92dfc6f4d2a1e22
>>>>>>> 1a7c5f4c8e01bcf1fb26d0dfe1a7e734c39fe395
    "last_modified": "2021-06-05T19:28:47+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-20-workbook4/",
    "title": "Workbook 4",
    "description": "Consolidating skills",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-03-20",
    "categories": [],
    "contents": "\r\nPreliminary data wrangling\r\n\r\n\r\n#Read data\r\nnz_0 <- readr::read_csv2(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/nz/nz.csv\"))\r\n\r\n#Re-level Kessler 6 variables\r\nf <- c(\"None Of The Time\",\"A Little Of The Time\",\"Some Of The Time\",  \"Most Of The Time\", \"All Of The Time\")\r\n\r\n#Transform data\r\nnz_1 <- nz_0 %>%\r\n  dplyr::mutate_if(is.character, factor) %>%\r\n  select(\r\n    -c(\r\n      SWB.Kessler01,\r\n      SWB.Kessler02,\r\n      SWB.Kessler03,\r\n      SWB.Kessler04,\r\n      SWB.Kessler05,\r\n      SWB.Kessler06\r\n    )\r\n  ) %>%\r\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\r\n  mutate(FeelHopeless = forcats::fct_relevel(FeelHopeless, f)) %>%\r\n  mutate(FeelDepressed = forcats::fct_relevel(FeelDepressed, f)) %>%\r\n  mutate(FeelRestless = forcats::fct_relevel(FeelRestless, f)) %>%\r\n  mutate(EverythingIsEffort = forcats::fct_relevel(EverythingIsEffort, f)) %>%\r\n  mutate(FeelWorthless = forcats::fct_relevel(FeelWorthless, f)) %>%\r\n  mutate(FeelNervous = forcats::fct_relevel(FeelNervous, f)) %>%\r\n  dplyr::mutate(Wave = as.factor(Wave)) %>%\r\n  dplyr::mutate(date = make_date(year = 2009, month = 6, day = 30) + TSCORE) \r\n\r\n\r\n\r\nQuestion 1\r\n\r\n\r\n#Check all variables containing \"hours\" string\r\nnz_1 %>%\r\n  select(contains(\"hours\")) %>%\r\n    glimpse()\r\n\r\n\r\nRows: 4,126\r\nColumns: 6\r\n$ HLTH.SleepHours <dbl> 6, 6, 6, 4, 7, 7, 6, 4, 7, 8, 7, 6, 7, 8, 6,~\r\n$ Hours.Internet  <dbl> 10, 5, 14, 15, 2, 2, 4, 4, 10, 14, 2, 2, 4, ~\r\n$ Hours.Exercise  <dbl> 14.0, 24.0, 7.0, 10.0, 5.0, 6.0, 1.0, 0.0, 1~\r\n$ Hours.Work      <dbl> 14, 0, 35, 65, 60, 50, 41, 42, 0, 0, 45, 40,~\r\n$ Hours.News      <dbl> 4.0, 4.0, 5.0, 4.0, 1.0, 2.0, 0.0, 0.0, 8.0,~\r\n$ HoursCharity    <dbl> 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 4.0, 0.0,~\r\n\r\n#Set above variables as integers\r\nnz_2 <- nz_1 %>%\r\n  dplyr::mutate(HLTH.SleepHours = as.integer(HLTH.SleepHours)) %>%\r\n  dplyr::mutate(Hours.Internet = as.integer(Hours.Internet)) %>%\r\n  dplyr::mutate(Hours.Exercise = as.integer(Hours.Exercise)) %>%\r\n  dplyr::mutate(Hours.Work = as.integer(Hours.Work)) %>%\r\n  dplyr::mutate(Hours.News = as.integer(Hours.News)) %>%\r\n  dplyr::mutate(HoursCharity = as.integer(HoursCharity))\r\n\r\n\r\n\r\nQuestion 2\r\n\r\n\r\n#Standardise and centre Pol.Orient; centre Age within decade brackets\r\nnz_3 <- nz_2 %>%\r\n  dplyr::mutate(Pol.Orient.Std = scale(Pol.Orient, scale = TRUE, center  = TRUE)) %>%\r\n  dplyr::mutate(Pol.Orient.Cntr = scale(Pol.Orient, scale = FALSE, center  = TRUE)) %>%\r\n  dplyr::mutate(Age.Brackets = cut(\r\n    Age,\r\n    breaks = c(16, 26, 36, 46, 56, 66, 76, Inf),\r\n    labels = c(\"16-25\", \"26-35\", \"36-45\", \"46-55\", \"56-65\", \"66-75\", \"76+\"), \r\n    right = TRUE)) %>%\r\n  group_by(Age.Brackets) %>%\r\n  dplyr::mutate(Age.Cntr = scale(Age, scale = FALSE, center  = TRUE)) %>%\r\n  ungroup()\r\n\r\n#Display head values of original and newly created variables\r\nnz_3 %>%\r\n  select(Pol.Orient, Pol.Orient.Std, Pol.Orient.Cntr, Age, Age.Brackets, Age.Cntr) %>%\r\n  head\r\n\r\n\r\n# A tibble: 6 x 6\r\n  Pol.Orient Pol.Orient.Std[,1] Pol.Orient.Cntr[,1]   Age Age.Brackets\r\n       <dbl>              <dbl>               <dbl> <dbl> <fct>       \r\n1          3             -0.420              -0.582    47 46-55       \r\n2          3             -0.420              -0.582    46 36-45       \r\n3          5              1.02                1.42     47 46-55       \r\n4          3             -0.420              -0.582    46 36-45       \r\n5          4              0.301               0.418    53 46-55       \r\n6          4              0.301               0.418    52 46-55       \r\n# ... with 1 more variable: Age.Cntr <dbl[,1]>\r\n\r\n#Select Hours.Exercise and filter by 2019 survey wave\r\nnz_3 %>%\r\n  filter(Wave==\"2019\") %>%\r\n  select(Hours.Exercise) %>%\r\n  head\r\n\r\n\r\n# A tibble: 6 x 1\r\n  Hours.Exercise\r\n           <int>\r\n1             14\r\n2              7\r\n3              5\r\n4              1\r\n5              1\r\n6              0\r\n\r\nQuestion 3\r\n\r\n\r\n#Slice dataframe by maximum daily response count for each survey wave\r\nnz_3 %>%\r\n  group_by(Wave) %>%\r\n  count(Day = floor_date(date, \"day\")) %>%\r\n  slice(which.max(n))\r\n\r\n\r\n# A tibble: 2 x 3\r\n# Groups:   Wave [2]\r\n  Wave  Day            n\r\n  <fct> <date>     <int>\r\n1 2018  2018-06-21   112\r\n2 2019  2019-12-03    54\r\n\r\nQuestion 4\r\n\r\n\r\n#Find interval between highest and second highest daily responses\r\nnz_3 %>%\r\n  count(Day = floor_date(date, \"day\")) %>%\r\n  arrange(desc(n)) %>%\r\n  dplyr::mutate(int = lubridate::interval(Day[1], Day[2])) %>%\r\n  dplyr::mutate(int = lubridate::int_standardize(int)) %>%\r\n  dplyr::mutate(tl = lubridate::time_length(int, \"day\")) %>%\r\n  dplyr::mutate(diff = n[1]-n[2]) %>%\r\n  slice(1:2)\r\n\r\n\r\n# A tibble: 2 x 5\r\n  Day            n int                               tl  diff\r\n  <date>     <int> <Interval>                     <dbl> <int>\r\n1 2018-06-21   112 2018-06-21 UTC--2018-06-22 UTC     1    19\r\n2 2018-06-22    93 2018-06-21 UTC--2018-06-22 UTC     1    19\r\n\r\nQuestion 5\r\n\r\n\r\n#Calculate age between two date/times in months.\r\nDOB = make_datetime(year = 1995, month = 12, day = 25, hour = 5, min = 2, sec = 22)\r\nAs_at_date = make_datetime(year = 2021, month = 3, day = 20, hour = 13, min = 22, sec = 4)\r\nage1 <- time_length(int_standardize(interval(DOB, As_at_date)), \"month\")\r\nage1\r\n\r\n\r\n[1] 302.8338\r\n\r\nQuestion 6\r\n\r\n\r\n#Separate Religion.Church into 3 factors and re-level variable\r\nnz_4 <- nz_3 %>%\r\n  dplyr::mutate(Church.Freq = cut(\r\n    Religion.Church,\r\n    breaks = c(-Inf, 1, 4, Inf),\r\n    labels = c(\"0\", \"1-3\", \"4+\"), \r\n    right = FALSE)) %>%\r\n  dplyr::mutate(Church.Freq = forcats::fct_relevel(Church.Freq, c(\"0\", \"1-3\", \"4+\")))\r\n\r\n\r\n\r\nQuestion 7\r\n\r\n\r\n#Factorise dates by month and label\r\nMnth <- factor(month(nz_4$date),\r\n               labels=c(\"Jan\",\r\n                        \"Feb\", \r\n                        \"Mar\",\r\n                        \"Apr\",\r\n                        \"May\",\r\n                        \"Jun\",\r\n                        \"Jul\",\r\n                        \"Aug\",\r\n                        \"Sep\",\r\n                        \"Oct\",\r\n                        \"Nov\",\r\n                        \"Dec\"))\r\n#Create table of average HLTH.SleepHours by month\r\ntable1::table1(~ HLTH.SleepHours | Mnth, data = nz_4, overall = FALSE)\r\n\r\n\r\n\r\nJan(N=265)\r\nFeb(N=174)\r\nMar(N=220)\r\nApr(N=81)\r\nMay(N=81)\r\nJun(N=725)\r\nJul(N=507)\r\nAug(N=240)\r\nSep(N=100)\r\nOct(N=772)\r\nNov(N=307)\r\nDec(N=654)\r\nHLTH.SleepHours\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nMean (SD)\r\n6.76 (1.16)\r\n6.60 (0.961)\r\n6.74 (1.13)\r\n7.21 (1.12)\r\n7.01 (1.09)\r\n6.88 (1.07)\r\n6.90 (1.11)\r\n6.95 (0.999)\r\n6.65 (1.08)\r\n6.84 (1.04)\r\n6.89 (1.22)\r\n6.83 (1.05)\r\nMedian [Min, Max]\r\n7.00 [3.00, 12.0]\r\n7.00 [4.00, 8.00]\r\n7.00 [2.00, 10.0]\r\n7.00 [4.00, 9.00]\r\n7.00 [5.00, 10.0]\r\n7.00 [2.00, 10.0]\r\n7.00 [3.00, 10.0]\r\n7.00 [4.00, 10.0]\r\n7.00 [4.00, 9.00]\r\n7.00 [3.00, 9.00]\r\n7.00 [2.00, 11.0]\r\n7.00 [4.00, 10.0]\r\nMissing\r\n12 (4.5%)\r\n9 (5.2%)\r\n9 (4.1%)\r\n3 (3.7%)\r\n4 (4.9%)\r\n34 (4.7%)\r\n28 (5.5%)\r\n10 (4.2%)\r\n7 (7.0%)\r\n30 (3.9%)\r\n10 (3.3%)\r\n25 (3.8%)\r\n\r\n\r\n#Plot average HLTH.SleepHours by month with 95% confidence intervals\r\nnz_4 %>%\r\n  select(Id, date, HLTH.SleepHours) %>%\r\n  mutate(months = month(date, label = TRUE)) %>%\r\n  group_by(months) %>%\r\n  summarise(\r\n    mn_Slphr =  mean(HLTH.SleepHours, na.rm = TRUE),\r\n    sd_Slphr =  sd(HLTH.SleepHours, na.rm = TRUE),\r\n    n_Slphr = n()\r\n  ) %>%\r\n  mutate(\r\n    se_Slphr = sd_Slphr / sqrt(n_Slphr),\r\n    lw_ci = mn_Slphr - qt(1 - (0.05 / 2), n_Slphr - 1) * se_Slphr,\r\n    up_ci = mn_Slphr + qt(1 - (0.05 / 2), n_Slphr - 1) * se_Slphr\r\n  ) %>%\r\n  ggplot(., aes(x = months, y = mn_Slphr, colour = mn_Slphr)) +\r\n  geom_errorbar(aes(ymin = lw_ci, ymax = up_ci), width = .1) +\r\n  geom_point(size = 3)  +\r\n  scale_y_continuous(limits = c(6,8)) + \r\n  theme_classic() + scale_fill_viridis_d()\r\n\r\n\r\n\r\n\r\nLooking more closely at the sample sizes for each month, there are fewer participants responding in those months which have wider confidence intervals. Smaller sample sizes increase the sampling error and accordingly the precision of our population estimates.\r\nQuestion 8\r\n\r\n\r\n#Plot correlations between Kessler 6 items, controlling for ID repeated measures\r\ncorr1 <- nz_4 %>%\r\n  select(\r\n    FeelHopeless,\r\n    FeelDepressed,\r\n    FeelRestless,\r\n    EverythingIsEffort,\r\n    FeelWorthless,\r\n    FeelNervous,\r\n    Id\r\n  ) %>%\r\n  mutate_all(., as.integer) %>%\r\n  mutate(Id = as.factor(Id)) %>%\r\n  correlation(partial = FALSE, multilevel = TRUE)\r\nsummary(corr1)\r\n\r\n\r\nParameter          | FeelNervous | FeelWorthless | EverythingIsEffort | FeelRestless | FeelDepressed\r\n----------------------------------------------------------------------------------------------------\r\nFeelHopeless       |     0.35*** |       0.56*** |            0.45*** |      0.34*** |       0.58***\r\nFeelDepressed      |     0.34*** |       0.59*** |            0.40*** |      0.30*** |              \r\nFeelRestless       |     0.38*** |       0.32*** |            0.38*** |              |              \r\nEverythingIsEffort |     0.33*** |       0.40*** |                    |              |              \r\nFeelWorthless      |     0.35*** |               |                    |              |              \r\n\r\nplot(corr1)\r\n\r\n\r\n\r\n\r\nThe correlation plot above displays the strength of correlations between the Kessler 6 items. The strongest correlations are between hopelessness, worthlessness, depression, and to a lesser extent, effortfulness. Some research has identified that a two-factor structure of depression and anxiety underlying the Kessler 6 is a better fitting model than a unidimensional structure (Bessaha, 2010; O’Connor & Parslow, 2010).\r\nQuestion 9\r\nSee attached Papaja file.\r\nQuestion 10\r\n\r\n\r\n#Plot correlations between Kessler 6 items, no controlling for ID repeated measures\r\ncorr2 <- nz_4 %>%\r\n  select(\r\n    FeelHopeless,\r\n    FeelDepressed,\r\n    FeelRestless,\r\n    EverythingIsEffort,\r\n    FeelWorthless,\r\n    FeelNervous,\r\n  ) %>%\r\n  mutate_all(., as.integer) %>%\r\n  correlation(partial = FALSE, multilevel = FALSE)\r\nsummary(corr2)\r\n\r\n\r\n# Correlation Matrix (pearson-method)\r\n\r\nParameter          | FeelNervous | FeelWorthless | EverythingIsEffort | FeelRestless | FeelDepressed\r\n----------------------------------------------------------------------------------------------------\r\nFeelHopeless       |     0.45*** |       0.65*** |            0.52*** |      0.43*** |       0.66***\r\nFeelDepressed      |     0.41*** |       0.67*** |            0.49*** |      0.38*** |              \r\nFeelRestless       |     0.47*** |       0.39*** |            0.46*** |              |              \r\nEverythingIsEffort |     0.43*** |       0.47*** |                    |              |              \r\nFeelWorthless      |     0.43*** |               |                    |              |              \r\n\r\np-value adjustment method: Holm (1979)\r\n\r\n#Use patchwork function to merge multilevel and single-level plots\r\nplot(corr1) / plot(corr2) + \r\n  plot_annotation(title = \"Plot of multilevel (a) and single-level (b) correlation\", tag_levels = 'a')\r\n\r\n\r\n\r\n\r\nReferences\r\n\r\n\r\n\r\nBessaha, M. L. (2010). Factor structure of the Kessler Psychological Distress scale (K6) among emerging adults. https://doi.org/10.1016/j.jad.2009.06.038\r\n\r\n\r\nO’Connor, D. W., & Parslow, R. A. (2010). Mental health scales and psychiatric diagnoses: Responses to GHQ-12, K-10 and CIDI across the lifespan. Journal of Affective Disorders, 121(3), 263–267. https://doi.org/10.1016/j.jad.2009.06.038\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-20-workbook4/workbook4_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2021-06-06T16:12:39+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-13-journal3/",
    "title": "Journal 3",
    "description": "Plotting to overthow the world with R.",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-03-13",
    "categories": [],
    "contents": "\r\nJoe commented this week that some of the data visualisation techniques might be too easy for those of us who have experience in using R. However, I take this as a personal challenge to extend myself where I can. I noticed several formatting issues when completing the workbook assessment, so I have tried to address these with functions such as str_wrap() to prevent the plot titles from running off the page, or scale_shape_manual() to account for more than 6 classes in the scatterplot. I also set chunk options to prevent annoying default warning messages from appearing. I would have liked to increase the height of some of the plots using the ggsave() function but wasn’t sure whether I needed to call the saved plot and how.\r\nI assisted Cam with an unexpected result in Question 2. Whereas I had faceted my plot by year of manufacture, he had set the colour for each class of the year variable in an unfaceted plot. Although this would theoretically work, he found that the graph displayed a legend with a colour-continuum across 1998-2004 including years between. After we puzzled together, I thought perhaps the column had been set as numerical rather than as a factor which would account for the continuous colour-coding. Using as.factor(year) resolved the issue.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-05-12T20:55:23+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-13-workbook3/",
    "title": "Workbook 3",
    "description": "Plots and graphs",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-03-13",
    "categories": [],
    "contents": "\r\nQuestion 1\r\nThe following code has a error preventing the data from being graphed correctly:\r\n\r\n\r\nggplot(data = mtcars) + \r\n  aes(mpg, wt, colour = factor(cyl))\r\n\r\n\r\n\r\n\r\nHere, the aes() function must be inside a geom_ function such as geom_point().\r\n\r\n\r\n#Plot mtcars dataset using scatterplot\r\nggplot(data = mtcars) +\r\n  geom_point(mapping = aes(mpg, wt, colour = factor(cyl)))\r\n\r\n\r\n\r\n\r\nQuestions 2 and 3\r\nWe can use the facet() function to compare different classes, such as year of manufacture. We can also set the plot axes start values to zero using expand_limits() and setting x and y to zero.\r\n\r\n\r\n#Plot mpg dataset and set axes start values to zero\r\nggplot(data = mpg) + \r\n  geom_point(mapping = aes(x = cty, y = hwy)) + \r\n  labs(title = str_wrap(\"Positive relationship between city and highway fuel efficiency by year of manufacture, in the mpg automobile dataset.\", 70)) +\r\n  facet_wrap(~ year, nrow = 2) +\r\n  xlab(\"City miles per gallon\") +\r\n  ylab(\"Highway miles per gallon\") +\r\n  expand_limits(x = 0, y = 0)\r\n\r\n\r\n\r\n\r\nQuestion 4\r\nThe benefit of setting the x and y axes to zero is that this avoids misleading readers about the scale of the changes represented by the data in the graph. Truncating axes effectively zooms into the graph, potentially making small variations appear larger than they actually are.\r\nA limitation of this approach is that sometimes small variations can be practically significant and therefore important to highlight. If we compare global temperature over a very long period of time, minute variations in temperature may be obscured by the relative stability over time. A plot of global temperature starting at zero may therefore underemphasise the effect of global warming.\r\nQuestion 5\r\nHere are two plots:\r\n\r\n\r\n#Store plot of mpg dataset using colours to differentiate car type\r\ng1 <- ggplot(data = mpg) + \r\n  geom_point(mapping = aes(x = displ, y = hwy, colour = class)) +\r\n  labs(title = str_wrap(\"Relationship between engine displacement and fuel efficiency in the mpg automobile dataset.\", 50)) +\r\n  xlab(\"Engine displacement in litres\") +\r\n  ylab(\"Highway miles per gallon\")\r\n\r\n#Store plot of mpg dataset using shapes to differentiate car type\r\ng2 <- ggplot(data = mpg) +\r\n  geom_point(mapping = aes(x = displ, y = hwy, shape = class)) +\r\n  scale_shape_manual(values=c(0, 1, 2, 3, 4, 5, 6, 7)) +\r\n  labs(title = str_wrap(\"Relationship between engine displacement and fuel efficiency in the mpg automobile dataset.\", 50)) +\r\n  xlab(\"Engine displacement in litres\") +\r\n  ylab(\"Highway miles per gallon\")\r\n\r\n#Call plots with annotation and tags\r\ng1 / g2 + plot_annotation(title = \"Which plot do you prefer and why?\", tag_levels = 'a')\r\n\r\n\r\n\r\n\r\nScatterplot (a) classifies car type by colour, while scatterplot (b) classifies by shape. I find that colour is much easier to intuit, for the simple reason that colours are easier to distinguish than shapes.\r\nQuestion 6\r\nAs previously described, we can add the facet() function to split our plot by a variable:\r\n\r\n\r\n#Store plot of mpg dataset faceted by car type\r\ng3 <- ggplot(data = mpg) +\r\n  geom_point(mapping = aes(x = displ, y = hwy, colour = class)) +\r\n  labs(title = str_wrap(\"Relationship between engine displacement and fuel efficiency in the mpg automobile dataset.\", 50)) +\r\n  facet_wrap(~ class, nrow = 2) +\r\n  xlab(\"Engine displacement in litres\") +\r\n  ylab(\"Highway miles per gallon\")\r\n#Call plot\r\ng3\r\n\r\n\r\n\r\n\r\nQuestion 7\r\nHere are two plots:\r\n\r\n\r\n#Call plots with annotation and tags\r\ng1 / g3 + plot_annotation(title = \"Which plot do you prefer and why?\", tag_levels = 'a')\r\n\r\n\r\n\r\n\r\nScatterplot (a) displays all car types in one plot, while scatterplot (b) facets each car type into separate plots. I would perhaps use these graphs in different situations - if it were necessary to compare car types, I would use the non-faceted view, while if it were necessary to examine the shape of the data for each individual car type then the faceted view would be easiest.\r\nQuestion 8\r\nWe can remove the legend by setting the legend.position to “none”:\r\n\r\n\r\n#Call plot and remove legend\r\ng3 + theme(legend.position = \"none\")\r\n\r\n\r\n\r\n\r\nQuestion 9\r\nIt is important to note which variables are listed in a dataset as factors or numerical values, as this affects how R will\r\n\r\n\r\n#Read dataset and scope data\r\nissp <- readr::read_csv2(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/issp.csv\"))\r\nhead(issp)\r\n\r\n\r\n# A tibble: 6 x 21\r\n     id   age male   eduyears nzeuro rightwing neg_ath  neg_bd  neg_ch\r\n  <dbl> <dbl> <chr>     <dbl>  <dbl>     <dbl> <chr>    <chr>   <chr> \r\n1     1    52 Not M~       16      1         4 Neither~ Somewh~ Somew~\r\n2     1    53 Not M~       13      1         3 Neither~ Neithe~ Neith~\r\n3     2    63 Not M~       10      1         4 Neither~ Neithe~ Somew~\r\n4     2    64 Not M~       12      1         1 Neither~ Neithe~ Somew~\r\n5     3    64 Male         NA      1         8 Neither~ Neithe~ Neith~\r\n6     3    65 Male         16      1         7 Neither~ Neithe~ Neith~\r\n# ... with 12 more variables: neg_hd <chr>, neg_jw <chr>,\r\n#   neg_ms <chr>, thr_ath <dbl>, thr_bd <chr>, thr_ch <chr>,\r\n#   thr_hd <chr>, thr_jw <chr>, rural <chr>, thr_ms <chr>,\r\n#   wave <dbl>, religiosity <dbl>\r\n\r\nstr(issp)\r\n\r\n\r\nspec_tbl_df [2,668 x 21] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\r\n $ id         : num [1:2668] 1 1 2 2 3 3 4 4 5 5 ...\r\n $ age        : num [1:2668] 52 53 63 64 64 65 29 30 41 42 ...\r\n $ male       : chr [1:2668] \"Not Male\" \"Not Male\" \"Not Male\" \"Not Male\" ...\r\n $ eduyears   : num [1:2668] 16 13 10 12 NA 16 11 13 11 14 ...\r\n $ nzeuro     : num [1:2668] 1 1 1 1 1 1 NA 1 1 1 ...\r\n $ rightwing  : num [1:2668] 4 3 4 1 8 7 8 8 6 7 ...\r\n $ neg_ath    : chr [1:2668] \"Neither negative nor positive\" \"Neither negative nor positive\" \"Neither negative nor positive\" \"Neither negative nor positive\" ...\r\n $ neg_bd     : chr [1:2668] \"Somewhat negative\" \"Neither negative nor positive\" \"Neither negative nor positive\" \"Neither negative nor positive\" ...\r\n $ neg_ch     : chr [1:2668] \"Somewhat negative\" \"Neither negative nor positive\" \"Somewhat negative\" \"Somewhat negative\" ...\r\n $ neg_hd     : chr [1:2668] \"Somewhat negative\" \"Somewhat negative\" \"Neither negative nor positive\" \"Neither negative nor positive\" ...\r\n $ neg_jw     : chr [1:2668] \"Somewhat negative\" \"Neither negative nor positive\" \"Neither negative nor positive\" \"Somewhat positive\" ...\r\n $ neg_ms     : chr [1:2668] \"Somewhat negative\" \"Somewhat negative\" \"Somewhat negative\" \"Somewhat negative\" ...\r\n $ thr_ath    : num [1:2668] 2 2 1 1 2 2 1 1 3 NA ...\r\n $ thr_bd     : chr [1:2668] \"Not very threatening\" \"Somewhat threatening\" \"Not threatening at all\" \"Not threatening at all\" ...\r\n $ thr_ch     : chr [1:2668] \"Not very threatening\" \"Somewhat threatening\" \"Somewhat threatening\" \"Somewhat threatening\" ...\r\n $ thr_hd     : chr [1:2668] \"Not very threatening\" \"Somewhat threatening\" \"Not threatening at all\" \"Not threatening at all\" ...\r\n $ thr_jw     : chr [1:2668] \"Not very threatening\" \"Somewhat threatening\" \"Not threatening at all\" \"Not threatening at all\" ...\r\n $ rural      : chr [1:2668] \"Not Rural\" \"Not Rural\" \"Not Rural\" \"Not Rural\" ...\r\n $ thr_ms     : chr [1:2668] \"Not very threatening\" \"Somewhat threatening\" \"Somewhat threatening\" \"Not very threatening\" ...\r\n $ wave       : num [1:2668] 2018 2019 2018 2019 2018 ...\r\n $ religiosity: num [1:2668] 6 4 3 4 4 6 7 7 5 5 ...\r\n - attr(*, \"spec\")=\r\n  .. cols(\r\n  ..   id = col_double(),\r\n  ..   age = col_double(),\r\n  ..   male = col_character(),\r\n  ..   eduyears = col_double(),\r\n  ..   nzeuro = col_double(),\r\n  ..   rightwing = col_double(),\r\n  ..   neg_ath = col_character(),\r\n  ..   neg_bd = col_character(),\r\n  ..   neg_ch = col_character(),\r\n  ..   neg_hd = col_character(),\r\n  ..   neg_jw = col_character(),\r\n  ..   neg_ms = col_character(),\r\n  ..   thr_ath = col_double(),\r\n  ..   thr_bd = col_character(),\r\n  ..   thr_ch = col_character(),\r\n  ..   thr_hd = col_character(),\r\n  ..   thr_jw = col_character(),\r\n  ..   rural = col_character(),\r\n  ..   thr_ms = col_character(),\r\n  ..   wave = col_double(),\r\n  ..   religiosity = col_double()\r\n  .. )\r\n\r\n#Re-code variables and save to new dataset\r\nip <- issp %>%\r\n  mutate(\r\n    id = factor(id),\r\n    thr_ath = as.factor(thr_ath),\r\n    thr_bd = as.factor(thr_bd),\r\n    thr_ch = as.factor(thr_ch),\r\n    thr_hd = as.factor(thr_hd),\r\n    thr_jw = as.factor(thr_jw),\r\n    thr_ms = as.factor(thr_ms),\r\n    neg_ath = as.factor(neg_ath),\r\n    neg_bd = as.factor(neg_bd),\r\n    neg_ch = as.factor(neg_ch),\r\n    neg_hd  = as.factor(neg_hd),\r\n    neg_jw = as.factor(neg_jw),\r\n    neg_ms = as.factor(neg_ms),\r\n    wave  = as.factor(wave),\r\n    nzeuro = as.factor(nzeuro),\r\n    eduyears = as.numeric(eduyears),\r\n    male = as.factor(male),\r\n    age = as.numeric(age),\r\n    rightwing = as.numeric(rightwing),\r\n    rural = as.factor(rural),\r\n    religiosity = as.numeric(religiosity)\r\n  )\r\n\r\n#Store plot of ip dataset using jitter to distinguish frequency of Likert responses\r\ng4 <- ggplot(data = ip, aes(y = as.numeric(thr_ms), x = religiosity, colour = wave))  +  geom_jitter(alpha = .1) + \r\n  geom_smooth(method = lm, fullrange = FALSE, alpha = 0.1) +\r\n  labs(title = str_wrap(\"Relationship between religiosity and perceived threat from Muslims in the ISSP dataset.\", 70)) +\r\n  xlab(\"Religiosity\") +\r\n  ylab(\"Perceived threat from Muslims\") +\r\n  scale_y_continuous(limits = c(0,4))\r\ng4\r\n\r\n\r\n\r\n\r\nQuestion 10\r\nWe can adjust the limits of the y axis using the scale_y_continuous() function.\r\n\r\n\r\n#Adjust y-axis limits\r\ng4 + scale_y_continuous(limits = c(1,4))\r\n\r\n\r\n\r\n\r\nQuestion 11\r\n\r\n\r\n#Plot cross-tabulation of ip dataset\r\nplot_xtab(\r\n    ip$thr_ms,\r\n    ip$wave,\r\n    show.total = FALSE,\r\n    show.n = FALSE,\r\n    geom.colors = c(\"lightgreen\", \"darkred\")\r\n    ) +\r\n  labs(title = str_wrap(\"Cross-tabulation of perceived threat from Muslims across study waves in the ISSP dataset.\", 65)) +\r\n  xlab(\"Threatened by Muslims\") +\r\n  ylab(\"Frequency\") +\r\n  scale_y_continuous(limits=c(0,1)) +\r\n  theme(plot.title = element_text(size=14), axis.text.x = element_text(angle = 20, hjust = 1)\r\n)\r\n\r\n\r\n\r\n\r\nThe original graph has been amended above as follows:\r\nAdded title.\r\nSeparated axis label code onto different lines.\r\nRemoved hashtag from scale_y_continuous() and theme() functions to display as code, not comment.\r\nResized y axis to reflect that scale of cross-tab plot cannot exceed 1.\r\nCombined plot title and axis text into a single theme() function.\r\nResized title to match other plots.\r\nSeparated final end parenthesis onto different line.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-13-workbook3/workbook3_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-06-06T14:35:26+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-06-journal2/",
    "title": "Journal 2",
    "description": "Fun and figures with R.",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-03-06",
    "categories": [],
    "contents": "\r\nThis week’s content was largely revision for me, as I was familiar with most of the coding topics covered. However, I appreciated some of the packages I had not yet used before such as sjPlot and Papaja. Because of my familiarity I wanted to extend myself with a few simple tricks, such as subsetting rows from a dataframe and customising histograms. I had some issues with setting up the working directory for the here() function to work, and then again with pushing updated files to GitHub. After some trial and error, I managed to synch up the files correctly.\r\nI also had the opportunity to offer my limited knowledge to other students. One student had difficulty running a linear model. After some questioning, it turned out that they had used the shortcut for running a selected line (Ctrl + Enter) rather than the entire current chunk (Ctrl + Shift + Enter). They were attempting to use the str() function on a model object that hadn’t yet been assigned. Once I showed them the difference, they were able to run the model.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-05-12T20:51:23+12:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-03-06-workbook2/",
    "title": "Workbook 2",
    "description": "Fun and figures with R",
    "author": [
      {
        "name": "Paul Edwards",
        "url": {}
      }
    ],
    "date": "2021-03-06",
    "categories": [],
    "contents": "\r\n\r\n\r\n#Mathematical calculations\r\n1+1\r\n\r\n\r\n\r\n[1] 2\r\n\r\n2-2\r\n\r\n\r\n[1] 0\r\n\r\n3*3\r\n\r\n\r\n[1] 9\r\n\r\n4/4\r\n\r\n\r\n[1] 1\r\n\r\n(5^5)^5\r\n\r\n\r\n[1] 2.980232e+17\r\n\r\n(6^6)^-6\r\n\r\n\r\n[1] 9.69516e-29\r\n\r\n7^-7\r\n\r\n\r\n[1] 1.214266e-06\r\n\r\n8.8*8.8\r\n\r\n\r\n[1] 77.44\r\n\r\n99+(9/9)\r\n\r\n\r\n[1] 100\r\n\r\ngo_factorial <- function(x) {\r\n  y <- 1\r\n  for (i in 1:x) {\r\n    y <- y * ((1:x)[i])\r\n  }\r\n  print(y)\r\n}\r\ngo_factorial(10)\r\n\r\n\r\n[1] 3628800\r\n\r\n\r\n\r\n#Square root function\r\nsqrt(324)\r\n\r\n\r\n\r\n[1] 18\r\n\r\n\r\n\r\n#Find row with max sepal length and subset dataframe\r\nmaxSL <- which.max(iris$Sepal.Length)\r\niris[maxSL,]\r\n\r\n\r\n\r\n    Sepal.Length Sepal.Width Petal.Length Petal.Width   Species\r\n132          7.9         3.8          6.4           2 virginica\r\n\r\n\r\n\r\n#Find row with min sepal length and subset dataframe\r\nminSL <- which.min(iris$Sepal.Length)\r\niris[minSL,]\r\n\r\n\r\n\r\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\r\n14          4.3           3          1.1         0.1  setosa\r\n\r\n\r\n\r\n#Calculate range between maximum and minimum sepal length\r\nrangeSL <- max(iris$Sepal.Length)-min(iris$Sepal.Length)\r\nrangeSL\r\n\r\n\r\n\r\n[1] 3.6\r\n\r\n\r\n\r\n#Construct dataframe with 7 columns and 100 randomly generated rows\r\ndf1 <- data.frame(\r\na = runif(100),\r\nb = runif(100),\r\nc = runif(100),\r\nd = runif(100),\r\ne = runif(100),\r\nf = runif(100),\r\ng = runif(100)\r\n)\r\nstr(df1)\r\n\r\n\r\n\r\n'data.frame':   100 obs. of  7 variables:\r\n $ a: num  0.658 0.876 0.37 0.502 0.372 ...\r\n $ b: num  0.495 0.314 0.157 0.14 0.858 ...\r\n $ c: num  0.6156 0.6613 0.7062 0.886 0.0708 ...\r\n $ d: num  0.739 0.213 0.208 1 0.316 ...\r\n $ e: num  0.145 0.817 0.394 0.485 0.202 ...\r\n $ f: num  0.7399 0.2254 0.3711 0.0626 0.68 ...\r\n $ g: num  0.0279 0.8114 0.1908 0.3993 0.1692 ...\r\n\r\n\r\n\r\n#Copy dataframe and rename columns\r\ndf2 <- df1\r\nnames(df2)[] <- c(\"Doc\",\"Grumpy\",\"Happy\",\"Sleepy\",\"Bashful\",\"Sneezy\",\"Dopey\")\r\nhead(df2)\r\n\r\n\r\n\r\n        Doc    Grumpy      Happy    Sleepy   Bashful     Sneezy\r\n1 0.6575520 0.4949787 0.61558464 0.7386390 0.1451720 0.73989898\r\n2 0.8756001 0.3143625 0.66130088 0.2128382 0.8169723 0.22542962\r\n3 0.3703949 0.1570729 0.70622551 0.2084606 0.3938718 0.37106843\r\n4 0.5016364 0.1396309 0.88604399 0.9997379 0.4848820 0.06257499\r\n5 0.3718757 0.8579842 0.07083648 0.3164299 0.2021289 0.68004481\r\n6 0.9719056 0.8119495 0.34832989 0.5000797 0.3030085 0.95936757\r\n       Dopey\r\n1 0.02794918\r\n2 0.81135776\r\n3 0.19079355\r\n4 0.39929049\r\n5 0.16917901\r\n6 0.56737081\r\n\r\n\r\n\r\n#Run linear regression model of height by weight\r\nmod1 <- lm(height ~ weight, data = women)\r\nsummary(mod1)\r\n\r\n\r\n\r\n\r\nCall:\r\nlm(formula = height ~ weight, data = women)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-0.83233 -0.26249  0.08314  0.34353  0.49790 \r\n\r\nCoefficients:\r\n             Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 25.723456   1.043746   24.64 2.68e-12 ***\r\nweight       0.287249   0.007588   37.85 1.09e-14 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 0.44 on 13 degrees of freedom\r\nMultiple R-squared:  0.991, Adjusted R-squared:  0.9903 \r\nF-statistic:  1433 on 1 and 13 DF,  p-value: 1.091e-14\r\n\r\n\r\n\r\n#Display model results in a table\r\nsjPlot::tab_model(mod1)\r\n\r\n\r\n\r\n \r\n\r\n\r\nheight\r\n\r\n\r\nPredictors\r\n\r\n\r\nEstimates\r\n\r\n\r\nCI\r\n\r\n\r\np\r\n\r\n\r\n(Intercept)\r\n\r\n\r\n25.72\r\n\r\n\r\n23.47 – 27.98\r\n\r\n\r\n<0.001\r\n\r\n\r\nweight\r\n\r\n\r\n0.29\r\n\r\n\r\n0.27 – 0.30\r\n\r\n\r\n<0.001\r\n\r\n\r\nObservations\r\n\r\n\r\n15\r\n\r\n\r\nR2 / R2 adjusted\r\n\r\n\r\n0.991 / 0.990\r\n\r\n\r\n#Display model results in a coefficient plot\r\nsjPlot::plot_model(mod1)\r\n\r\n\r\n\r\n\r\n\r\n\r\n#Display model results in a prediction plot with observations\r\nplot1 <- ggeffects::ggpredict(mod1, terms = \"weight\")\r\nplot(plot1,\r\n     add.data = TRUE,\r\n     dot.alpha = .8,\r\n     jitter = .2)\r\n\r\n\r\n\r\n\r\n\r\n#Example calculation\r\nsum(women$weight > 140) / length(women$weight)\r\n\r\n\r\n\r\n[1] 0.4\r\n\r\nThis calculation is taking the number of women who weigh over 140 pounds, divided by the total number of women in the sample i.e. the proportion of women who weigh over 140 pounds.\r\n\r\n\r\n#Calculate proportion of women over the average weight\r\nmwgt <- mean(women$weight)\r\nsum(women$weight > mwgt) / length(women$weight)\r\n\r\n\r\n\r\n[1] 0.4666667\r\n\r\n46.67% of women weigh over the mean weight of women in this sample.\r\n\r\n\r\n#Set plot area to row of 3 graphs\r\npar(mfrow=c(1,3))\r\n#Plot histograms with varying break length\r\nhist(iris$Petal.Length,\r\n main=\"Histogram of Iris Petal Length\",\r\n xlab=\"Petal Length\")\r\nhist(iris$Petal.Length,\r\n main=\"Histogram of Iris Petal Length\",\r\n xlab=\"Petal Length\",\r\n breaks=seq(1,7,2))\r\nhist(iris$Petal.Length,\r\n main=\"Histogram of Iris Petal Length\",\r\n xlab=\"Petal Length\",\r\n breaks=seq(1,7,0.2))\r\n\r\n\r\n\r\nHistogram breaks determine the size and number of bins, or columns in which data is displayed. Too few bins and interesting points within the distribution will be lost; too many bins will result in noisy, meaningless data.\r\n\r\n\r\n#Identify coding error\r\nmh <- mean(women$height)\r\nsum(women$weight > mh) / length(women$height)\r\n\r\n\r\n\r\n[1] 1\r\n\r\nThis code has the height variable mis-typed. It is attempting to sum the weight of women with weights greater than the mean height. The two different scales mean that all women have weights greater than the mean height.\r\n\r\n\r\n#Transpose and rename dataframe columns\r\nwomen2 <- data.frame(women[,c(2,1)])\r\nnames(women2)[] <- c(\"w\",\"h\")\r\nstr(women2)\r\n\r\n\r\n\r\n'data.frame':   15 obs. of  2 variables:\r\n $ w: num  115 117 120 123 126 129 132 135 139 142 ...\r\n $ h: num  58 59 60 61 62 63 64 65 66 67 ...\r\n\r\n\r\n\r\n#Read data\r\nlibrary(readr)\r\ntestdata <- readr::read_csv(url(\"https://raw.githubusercontent.com/go-bayes/psych-447/main/data/testdata1.csv\"))\r\nstr(testdata)\r\n\r\n\r\n\r\nspec_tbl_df [100 x 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\r\n $ id    : num [1:100] 1 2 3 4 5 6 7 8 9 10 ...\r\n $ weight: num [1:100] 80.5 87.8 95.7 74.6 68.3 ...\r\n $ height: num [1:100] 153 182 188 142 137 ...\r\n - attr(*, \"spec\")=\r\n  .. cols(\r\n  ..   id = col_double(),\r\n  ..   weight = col_double(),\r\n  ..   height = col_double()\r\n  .. )\r\n\r\n#Save data\r\nlibrary(here)\r\nsaveRDS(testdata, here::here(\"_posts/2021-03-06-workbook2/workbook2_files\", \"td.RDS\"))\r\n\r\ntd <- readRDS(here::here(\"_posts/2021-03-06-workbook2/workbook2_files\", \"td.RDS\"))\r\nstr(td)\r\n\r\n\r\nspec_tbl_df [100 x 3] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\r\n $ id    : num [1:100] 1 2 3 4 5 6 7 8 9 10 ...\r\n $ weight: num [1:100] 80.5 87.8 95.7 74.6 68.3 ...\r\n $ height: num [1:100] 153 182 188 142 137 ...\r\n - attr(*, \"spec\")=\r\n  .. cols(\r\n  ..   id = col_double(),\r\n  ..   weight = col_double(),\r\n  ..   height = col_double()\r\n  .. )\r\n\r\n#Run linear regression model of height by weight\r\nmod2 <- lm(height ~ weight, data = td)\r\nsummary(mod2)\r\n\r\n\r\n\r\nCall:\r\nlm(formula = height ~ weight, data = td)\r\n\r\nResiduals:\r\n     Min       1Q   Median       3Q      Max \r\n-25.4761  -6.2215  -0.1467   4.8392  23.9751 \r\n\r\nCoefficients:\r\n            Estimate Std. Error t value Pr(>|t|)    \r\n(Intercept) 12.74337    3.64557   3.496 0.000712 ***\r\nweight       1.87029    0.04432  42.201  < 2e-16 ***\r\n---\r\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\r\n\r\nResidual standard error: 9.683 on 98 degrees of freedom\r\nMultiple R-squared:  0.9478,    Adjusted R-squared:  0.9473 \r\nF-statistic:  1781 on 1 and 98 DF,  p-value: < 2.2e-16\r\n\r\n#Display model results in a coefficient plot\r\nsjPlot::plot_model(mod2)\r\n\r\n\r\n\r\n#Display model results in a prediction plot with observations\r\nplot2 <- ggeffects::ggpredict(mod2, terms = \"weight\")\r\nplot(plot2,\r\n        add.data = TRUE,\r\n        dot.alpha = .8,\r\n        jitter = .2)\r\n\r\n\r\n\r\n\r\nThe intercept represents the predicted value of height when weight is zero. Here, the intercept is nonsensical as it would mean a height of 12.7cm given a weight of 0kg, which is generally not possible unless you are in space.\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-03-06-workbook2/workbook2_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2021-06-06T14:31:23+12:00",
    "input_file": {}
  }
]
